{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function to drop weight columns\n",
    "def drop_weights(data_frame):\n",
    "    coltodrop = []\n",
    "    for i in range(1, 81):\n",
    "        coltodrop.append('WGTP' + str(i))\n",
    "    return data_frame.drop(coltodrop, axis = 1)\n",
    "\n",
    "# Function to drop allocation columns\n",
    "def drop_allocations(data_frame):\n",
    "    col = data_frame.columns\n",
    "    coltodrop = []\n",
    "    for c in col:\n",
    "        if (c[0]== 'F') & (c[-1] == 'P') & \\\n",
    "           (c != 'FULFP') & (c != 'FULP') & (c != 'FINCP'): \n",
    "            coltodrop.append(c)\n",
    "    return data_frame.drop(coltodrop, axis = 1)\n",
    "\n",
    "# Function to recode the categorical features \n",
    "def recode(data_frame):\n",
    "    # Binary categories\n",
    "    dict_bin = {0: 1, 1: 2}\n",
    "    bin_cols = ['HUGCL', 'NPP', 'NR', 'PSF', 'R18']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {1: 2, 2: 1}\n",
    "    bin_cols = ['BROADBND', 'COMPOTHX', 'DIALUP', 'HISPEED', 'LAPTOP', 'OTHSVCEX', \\\n",
    "                'SATELLITE', 'SMARTPHONE', 'TABLET', 'TEL', 'FS', 'BATH', 'REFR', \\\n",
    "                'SINK', 'STOV', 'KIT', 'RNTM']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {0: 2, 1: 1}\n",
    "    bin_cols = ['SRNT', 'SVAL']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    # Three categories\n",
    "    dict_bin = {1: 2, 2: 1, 9:0}\n",
    "    bin_cols = ['RWAT', 'RWATPR', 'PLM', 'PLMPRP', 'HOTWAT']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {1: 2, 2: 1, 3: 3}\n",
    "    bin_cols = ['ELEFP', 'FULFP', 'GASFP', 'WATFP']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    return data_frame\n",
    "    \n",
    "# Customized imputer function\n",
    "# Since fill_value is a constant, we apply the function to the entire dataset\n",
    "def imputer(data_frame):\n",
    "    imptr = SimpleImputer(strategy = 'constant', fill_value = 0)\n",
    "    relevant_cols = ['SERIALNO', 'REGION', 'DIVISION', 'ST', 'PUMA', 'FS', \\\n",
    "                     'HINCP', 'ADJINC', 'WIF', 'WORKSTAT', 'VEH', 'NP', 'HHL', \\\n",
    "                     'FPARC', 'HHT', 'HUGCL', 'HUPAC', 'LNGI', 'MULTG', 'NPF', \\\n",
    "                     'NPP', 'NR', 'NRC', 'PARTNER', 'PSF', 'R18', 'R65', 'SSMC', \\\n",
    "                     'ACCESS', 'BROADBND', 'COMPOTHX', 'DIALUP', 'HISPEED', 'LAPTOP', \\\n",
    "                     'OTHSVCEX', 'SATELLITE', 'SMARTPHONE', 'TABLET', 'TEL', \\\n",
    "                     'TYPE', 'BATH', 'BDSP', 'BLD', 'REFR', 'RMSP', 'RWAT', \\\n",
    "                     'RWATPR', 'SINK', 'STOV', 'TEN', 'KIT', 'MV', 'PLM', 'PLMPRP', \\\n",
    "                     'HOTWAT', 'SRNT', 'SVAL', 'YBL', 'CONP', 'ELEFP', 'ELEP', \\\n",
    "                     'FULFP', 'FULP', 'GASFP', 'GASP', 'HFL', 'INSP', 'MHP', \\\n",
    "                     'RNTM', 'RNTP', 'WATFP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "    data_frame = data_frame[relevant_cols]\n",
    "    data_frame = data_frame.dropna(subset = ['FS', 'HINCP'])    \n",
    "    data_frame = imptr.fit_transform(data_frame)\n",
    "    return pd.DataFrame(data_frame, columns = relevant_cols)\n",
    "    \n",
    "# Import the nationwide data-it is presented in two files \n",
    "file_dir = '/Users/flatironschol/FIS-Projects/Module5/data/'\n",
    "df = pd.read_csv('psam_h06.csv')\n",
    "df = drop_weights(df)\n",
    "df = drop_allocations(df)\n",
    "df = recode(df)\n",
    "df = imputer(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More pre-processing: scaling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test train split.\n",
      "Finished scaling.\n",
      "Encoded PUMA_101.\n",
      "Encoded PUMA_102.\n",
      "Encoded PUMA_103.\n",
      "Encoded PUMA_104.\n",
      "Encoded PUMA_105.\n",
      "Encoded PUMA_106.\n",
      "Encoded PUMA_107.\n",
      "Encoded PUMA_108.\n",
      "Encoded PUMA_109.\n",
      "Encoded PUMA_110.\n",
      "Encoded PUMA_300.\n",
      "Encoded PUMA_701.\n",
      "Encoded PUMA_702.\n",
      "Encoded PUMA_1100.\n",
      "Encoded PUMA_1301.\n",
      "Encoded PUMA_1302.\n",
      "Encoded PUMA_1303.\n",
      "Encoded PUMA_1304.\n",
      "Encoded PUMA_1305.\n",
      "Encoded PUMA_1306.\n",
      "Encoded PUMA_1307.\n",
      "Encoded PUMA_1308.\n",
      "Encoded PUMA_1309.\n",
      "Encoded PUMA_1500.\n",
      "Encoded PUMA_1700.\n",
      "Encoded PUMA_1901.\n",
      "Encoded PUMA_1902.\n",
      "Encoded PUMA_1903.\n",
      "Encoded PUMA_1904.\n",
      "Encoded PUMA_1905.\n",
      "Encoded PUMA_1906.\n",
      "Encoded PUMA_1907.\n",
      "Encoded PUMA_2300.\n",
      "Encoded PUMA_2500.\n",
      "Encoded PUMA_2901.\n",
      "Encoded PUMA_2902.\n",
      "Encoded PUMA_2903.\n",
      "Encoded PUMA_2904.\n",
      "Encoded PUMA_2905.\n",
      "Encoded PUMA_3100.\n",
      "Encoded PUMA_3300.\n",
      "Encoded PUMA_3701.\n",
      "Encoded PUMA_3702.\n",
      "Encoded PUMA_3703.\n",
      "Encoded PUMA_3704.\n",
      "Encoded PUMA_3705.\n",
      "Encoded PUMA_3706.\n",
      "Encoded PUMA_3707.\n",
      "Encoded PUMA_3708.\n",
      "Encoded PUMA_3709.\n",
      "Encoded PUMA_3710.\n",
      "Encoded PUMA_3711.\n",
      "Encoded PUMA_3712.\n",
      "Encoded PUMA_3713.\n",
      "Encoded PUMA_3714.\n",
      "Encoded PUMA_3715.\n",
      "Encoded PUMA_3716.\n",
      "Encoded PUMA_3717.\n",
      "Encoded PUMA_3718.\n",
      "Encoded PUMA_3719.\n",
      "Encoded PUMA_3720.\n",
      "Encoded PUMA_3721.\n",
      "Encoded PUMA_3722.\n",
      "Encoded PUMA_3723.\n",
      "Encoded PUMA_3724.\n",
      "Encoded PUMA_3725.\n",
      "Encoded PUMA_3726.\n",
      "Encoded PUMA_3727.\n",
      "Encoded PUMA_3728.\n",
      "Encoded PUMA_3729.\n",
      "Encoded PUMA_3730.\n",
      "Encoded PUMA_3731.\n",
      "Encoded PUMA_3732.\n",
      "Encoded PUMA_3733.\n",
      "Encoded PUMA_3734.\n",
      "Encoded PUMA_3735.\n",
      "Encoded PUMA_3736.\n",
      "Encoded PUMA_3737.\n",
      "Encoded PUMA_3738.\n",
      "Encoded PUMA_3739.\n",
      "Encoded PUMA_3740.\n",
      "Encoded PUMA_3741.\n",
      "Encoded PUMA_3742.\n",
      "Encoded PUMA_3743.\n",
      "Encoded PUMA_3744.\n",
      "Encoded PUMA_3745.\n",
      "Encoded PUMA_3746.\n",
      "Encoded PUMA_3747.\n",
      "Encoded PUMA_3748.\n",
      "Encoded PUMA_3749.\n",
      "Encoded PUMA_3750.\n",
      "Encoded PUMA_3751.\n",
      "Encoded PUMA_3752.\n",
      "Encoded PUMA_3753.\n",
      "Encoded PUMA_3754.\n",
      "Encoded PUMA_3755.\n",
      "Encoded PUMA_3756.\n",
      "Encoded PUMA_3757.\n",
      "Encoded PUMA_3758.\n",
      "Encoded PUMA_3759.\n",
      "Encoded PUMA_3760.\n",
      "Encoded PUMA_3761.\n",
      "Encoded PUMA_3762.\n",
      "Encoded PUMA_3763.\n",
      "Encoded PUMA_3764.\n",
      "Encoded PUMA_3765.\n",
      "Encoded PUMA_3766.\n",
      "Encoded PUMA_3767.\n",
      "Encoded PUMA_3768.\n",
      "Encoded PUMA_3769.\n",
      "Encoded PUMA_3900.\n",
      "Encoded PUMA_4101.\n",
      "Encoded PUMA_4102.\n",
      "Encoded PUMA_4701.\n",
      "Encoded PUMA_4702.\n",
      "Encoded PUMA_5301.\n",
      "Encoded PUMA_5302.\n",
      "Encoded PUMA_5303.\n",
      "Encoded PUMA_5500.\n",
      "Encoded PUMA_5700.\n",
      "Encoded PUMA_5901.\n",
      "Encoded PUMA_5902.\n",
      "Encoded PUMA_5903.\n",
      "Encoded PUMA_5904.\n",
      "Encoded PUMA_5905.\n",
      "Encoded PUMA_5906.\n",
      "Encoded PUMA_5907.\n",
      "Encoded PUMA_5908.\n",
      "Encoded PUMA_5909.\n",
      "Encoded PUMA_5910.\n",
      "Encoded PUMA_5911.\n",
      "Encoded PUMA_5912.\n",
      "Encoded PUMA_5913.\n",
      "Encoded PUMA_5914.\n",
      "Encoded PUMA_5915.\n",
      "Encoded PUMA_5916.\n",
      "Encoded PUMA_5917.\n",
      "Encoded PUMA_5918.\n",
      "Encoded PUMA_6101.\n",
      "Encoded PUMA_6102.\n",
      "Encoded PUMA_6103.\n",
      "Encoded PUMA_6501.\n",
      "Encoded PUMA_6502.\n",
      "Encoded PUMA_6503.\n",
      "Encoded PUMA_6504.\n",
      "Encoded PUMA_6505.\n",
      "Encoded PUMA_6506.\n",
      "Encoded PUMA_6507.\n",
      "Encoded PUMA_6508.\n",
      "Encoded PUMA_6509.\n",
      "Encoded PUMA_6510.\n",
      "Encoded PUMA_6511.\n",
      "Encoded PUMA_6512.\n",
      "Encoded PUMA_6513.\n",
      "Encoded PUMA_6514.\n",
      "Encoded PUMA_6515.\n",
      "Encoded PUMA_6701.\n",
      "Encoded PUMA_6702.\n",
      "Encoded PUMA_6703.\n",
      "Encoded PUMA_6704.\n",
      "Encoded PUMA_6705.\n",
      "Encoded PUMA_6706.\n",
      "Encoded PUMA_6707.\n",
      "Encoded PUMA_6708.\n",
      "Encoded PUMA_6709.\n",
      "Encoded PUMA_6710.\n",
      "Encoded PUMA_6711.\n",
      "Encoded PUMA_6712.\n",
      "Encoded PUMA_7101.\n",
      "Encoded PUMA_7102.\n",
      "Encoded PUMA_7103.\n",
      "Encoded PUMA_7104.\n",
      "Encoded PUMA_7105.\n",
      "Encoded PUMA_7106.\n",
      "Encoded PUMA_7107.\n",
      "Encoded PUMA_7108.\n",
      "Encoded PUMA_7109.\n",
      "Encoded PUMA_7110.\n",
      "Encoded PUMA_7111.\n",
      "Encoded PUMA_7112.\n",
      "Encoded PUMA_7113.\n",
      "Encoded PUMA_7114.\n",
      "Encoded PUMA_7115.\n",
      "Encoded PUMA_7301.\n",
      "Encoded PUMA_7302.\n",
      "Encoded PUMA_7303.\n",
      "Encoded PUMA_7304.\n",
      "Encoded PUMA_7305.\n",
      "Encoded PUMA_7306.\n",
      "Encoded PUMA_7307.\n",
      "Encoded PUMA_7308.\n",
      "Encoded PUMA_7309.\n",
      "Encoded PUMA_7310.\n",
      "Encoded PUMA_7311.\n",
      "Encoded PUMA_7312.\n",
      "Encoded PUMA_7313.\n",
      "Encoded PUMA_7314.\n",
      "Encoded PUMA_7315.\n",
      "Encoded PUMA_7316.\n",
      "Encoded PUMA_7317.\n",
      "Encoded PUMA_7318.\n",
      "Encoded PUMA_7319.\n",
      "Encoded PUMA_7320.\n",
      "Encoded PUMA_7321.\n",
      "Encoded PUMA_7322.\n",
      "Encoded PUMA_7501.\n",
      "Encoded PUMA_7502.\n",
      "Encoded PUMA_7503.\n",
      "Encoded PUMA_7504.\n",
      "Encoded PUMA_7505.\n",
      "Encoded PUMA_7506.\n",
      "Encoded PUMA_7507.\n",
      "Encoded PUMA_7701.\n",
      "Encoded PUMA_7702.\n",
      "Encoded PUMA_7703.\n",
      "Encoded PUMA_7704.\n",
      "Encoded PUMA_7901.\n",
      "Encoded PUMA_7902.\n",
      "Encoded PUMA_8101.\n",
      "Encoded PUMA_8102.\n",
      "Encoded PUMA_8103.\n",
      "Encoded PUMA_8104.\n",
      "Encoded PUMA_8105.\n",
      "Encoded PUMA_8106.\n",
      "Encoded PUMA_8301.\n",
      "Encoded PUMA_8302.\n",
      "Encoded PUMA_8303.\n",
      "Encoded PUMA_8501.\n",
      "Encoded PUMA_8502.\n",
      "Encoded PUMA_8503.\n",
      "Encoded PUMA_8504.\n",
      "Encoded PUMA_8505.\n",
      "Encoded PUMA_8506.\n",
      "Encoded PUMA_8507.\n",
      "Encoded PUMA_8508.\n",
      "Encoded PUMA_8509.\n",
      "Encoded PUMA_8510.\n",
      "Encoded PUMA_8511.\n",
      "Encoded PUMA_8512.\n",
      "Encoded PUMA_8513.\n",
      "Encoded PUMA_8514.\n",
      "Encoded PUMA_8701.\n",
      "Encoded PUMA_8702.\n",
      "Encoded PUMA_8900.\n",
      "Encoded PUMA_9501.\n",
      "Encoded PUMA_9502.\n",
      "Encoded PUMA_9503.\n",
      "Encoded PUMA_9701.\n",
      "Encoded PUMA_9702.\n",
      "Encoded PUMA_9703.\n",
      "Encoded PUMA_9901.\n",
      "Encoded PUMA_9902.\n",
      "Encoded PUMA_9903.\n",
      "Encoded PUMA_9904.\n",
      "Encoded PUMA_10100.\n",
      "Encoded PUMA_10701.\n",
      "Encoded PUMA_10702.\n",
      "Encoded PUMA_10703.\n",
      "Encoded PUMA_11101.\n",
      "Encoded PUMA_11102.\n",
      "Encoded PUMA_11103.\n",
      "Encoded PUMA_11104.\n",
      "Encoded PUMA_11105.\n",
      "Encoded PUMA_11106.\n",
      "Encoded PUMA_11300.\n",
      "Encoded ADJINC_1013097.\n",
      "Encoded WIF_0.\n",
      "Encoded WIF_1.0.\n",
      "Encoded WIF_2.0.\n",
      "Encoded WIF_3.0.\n",
      "Encoded WORKSTAT_0.\n",
      "Encoded WORKSTAT_1.0.\n",
      "Encoded WORKSTAT_2.0.\n",
      "Encoded WORKSTAT_3.0.\n",
      "Encoded WORKSTAT_4.0.\n",
      "Encoded WORKSTAT_5.0.\n",
      "Encoded WORKSTAT_6.0.\n",
      "Encoded WORKSTAT_7.0.\n",
      "Encoded WORKSTAT_8.0.\n",
      "Encoded WORKSTAT_9.0.\n",
      "Encoded WORKSTAT_10.0.\n",
      "Encoded WORKSTAT_11.0.\n",
      "Encoded WORKSTAT_12.0.\n",
      "Encoded WORKSTAT_13.0.\n",
      "Encoded WORKSTAT_14.0.\n",
      "Encoded WORKSTAT_15.0.\n",
      "Encoded HHL_1.0.\n",
      "Encoded HHL_2.0.\n",
      "Encoded HHL_3.0.\n",
      "Encoded HHL_4.0.\n",
      "Encoded HHL_5.0.\n",
      "Encoded FPARC_0.\n",
      "Encoded FPARC_1.0.\n",
      "Encoded FPARC_2.0.\n",
      "Encoded FPARC_3.0.\n",
      "Encoded FPARC_4.0.\n",
      "Encoded HHT_1.0.\n",
      "Encoded HHT_2.0.\n",
      "Encoded HHT_3.0.\n",
      "Encoded HHT_4.0.\n",
      "Encoded HHT_5.0.\n",
      "Encoded HHT_6.0.\n",
      "Encoded HHT_7.0.\n",
      "Encoded HUGCL_1.0.\n",
      "Encoded HUGCL_2.0.\n",
      "Encoded HUPAC_1.0.\n",
      "Encoded HUPAC_2.0.\n",
      "Encoded HUPAC_3.0.\n",
      "Encoded HUPAC_4.0.\n",
      "Encoded LNGI_1.0.\n",
      "Encoded LNGI_2.0.\n",
      "Encoded MULTG_1.0.\n",
      "Encoded MULTG_2.0.\n",
      "Encoded NPP_1.0.\n",
      "Encoded NPP_2.0.\n",
      "Encoded NR_1.0.\n",
      "Encoded NR_2.0.\n",
      "Encoded PARTNER_0.0.\n",
      "Encoded PARTNER_1.0.\n",
      "Encoded PARTNER_2.0.\n",
      "Encoded PARTNER_3.0.\n",
      "Encoded PARTNER_4.0.\n",
      "Encoded PSF_1.0.\n",
      "Encoded PSF_2.0.\n",
      "Encoded R18_1.0.\n",
      "Encoded R18_2.0.\n",
      "Encoded R65_0.0.\n",
      "Encoded R65_1.0.\n",
      "Encoded R65_2.0.\n",
      "Encoded SSMC_0.0.\n",
      "Encoded SSMC_1.0.\n",
      "Encoded SSMC_2.0.\n",
      "Encoded ACCESS_1.0.\n",
      "Encoded ACCESS_2.0.\n",
      "Encoded ACCESS_3.0.\n",
      "Encoded BROADBND_0.\n",
      "Encoded BROADBND_1.0.\n",
      "Encoded BROADBND_2.0.\n",
      "Encoded COMPOTHX_1.0.\n",
      "Encoded COMPOTHX_2.0.\n",
      "Encoded DIALUP_0.\n",
      "Encoded DIALUP_1.0.\n",
      "Encoded DIALUP_2.0.\n",
      "Encoded HISPEED_0.\n",
      "Encoded HISPEED_1.0.\n",
      "Encoded HISPEED_2.0.\n",
      "Encoded LAPTOP_1.0.\n",
      "Encoded LAPTOP_2.0.\n",
      "Encoded OTHSVCEX_0.\n",
      "Encoded OTHSVCEX_1.0.\n",
      "Encoded OTHSVCEX_2.0.\n",
      "Encoded SATELLITE_0.\n",
      "Encoded SATELLITE_1.0.\n",
      "Encoded SATELLITE_2.0.\n",
      "Encoded SMARTPHONE_1.0.\n",
      "Encoded SMARTPHONE_2.0.\n",
      "Encoded TABLET_1.0.\n",
      "Encoded TABLET_2.0.\n",
      "Encoded TEL_1.0.\n",
      "Encoded TEL_2.0.\n",
      "Encoded TYPE_1.\n",
      "Encoded BATH_1.0.\n",
      "Encoded BATH_2.0.\n",
      "Encoded REFR_1.0.\n",
      "Encoded REFR_2.0.\n",
      "Encoded RWAT_1.0.\n",
      "Encoded RWAT_2.0.\n",
      "Encoded SINK_1.0.\n",
      "Encoded SINK_2.0.\n",
      "Encoded STOV_1.0.\n",
      "Encoded STOV_2.0.\n",
      "Encoded TEN_1.0.\n",
      "Encoded TEN_2.0.\n",
      "Encoded TEN_3.0.\n",
      "Encoded TEN_4.0.\n",
      "Encoded KIT_1.0.\n",
      "Encoded KIT_2.0.\n",
      "Encoded MV_1.0.\n",
      "Encoded MV_2.0.\n",
      "Encoded MV_3.0.\n",
      "Encoded MV_4.0.\n",
      "Encoded MV_5.0.\n",
      "Encoded MV_6.0.\n",
      "Encoded MV_7.0.\n",
      "Encoded PLM_1.0.\n",
      "Encoded PLM_2.0.\n",
      "Encoded SRNT_1.0.\n",
      "Encoded SRNT_2.0.\n",
      "Encoded SVAL_1.0.\n",
      "Encoded SVAL_2.0.\n",
      "Encoded ELEFP_1.0.\n",
      "Encoded ELEFP_2.0.\n",
      "Encoded ELEFP_3.0.\n",
      "Encoded FULFP_1.0.\n",
      "Encoded FULFP_2.0.\n",
      "Encoded FULFP_3.0.\n",
      "Encoded GASFP_0.\n",
      "Encoded GASFP_1.0.\n",
      "Encoded GASFP_2.0.\n",
      "Encoded GASFP_3.0.\n",
      "Encoded HFL_1.0.\n",
      "Encoded HFL_2.0.\n",
      "Encoded HFL_3.0.\n",
      "Encoded HFL_4.0.\n",
      "Encoded HFL_5.0.\n",
      "Encoded HFL_6.0.\n",
      "Encoded HFL_7.0.\n",
      "Encoded HFL_8.0.\n",
      "Encoded HFL_9.0.\n",
      "Encoded RNTM_0.\n",
      "Encoded RNTM_1.0.\n",
      "Encoded RNTM_2.0.\n",
      "Encoded WATFP_1.0.\n",
      "Encoded WATFP_2.0.\n",
      "Encoded WATFP_3.0.\n",
      "Encoded PUMA_101.\n",
      "Encoded PUMA_102.\n",
      "Encoded PUMA_103.\n",
      "Encoded PUMA_104.\n",
      "Encoded PUMA_105.\n",
      "Encoded PUMA_106.\n",
      "Encoded PUMA_107.\n",
      "Encoded PUMA_108.\n",
      "Encoded PUMA_109.\n",
      "Encoded PUMA_110.\n",
      "Encoded PUMA_300.\n",
      "Encoded PUMA_701.\n",
      "Encoded PUMA_702.\n",
      "Encoded PUMA_1100.\n",
      "Encoded PUMA_1301.\n",
      "Encoded PUMA_1302.\n",
      "Encoded PUMA_1303.\n",
      "Encoded PUMA_1304.\n",
      "Encoded PUMA_1305.\n",
      "Encoded PUMA_1306.\n",
      "Encoded PUMA_1307.\n",
      "Encoded PUMA_1308.\n",
      "Encoded PUMA_1309.\n",
      "Encoded PUMA_1500.\n",
      "Encoded PUMA_1700.\n",
      "Encoded PUMA_1901.\n",
      "Encoded PUMA_1902.\n",
      "Encoded PUMA_1903.\n",
      "Encoded PUMA_1904.\n",
      "Encoded PUMA_1905.\n",
      "Encoded PUMA_1906.\n",
      "Encoded PUMA_1907.\n",
      "Encoded PUMA_2300.\n",
      "Encoded PUMA_2500.\n",
      "Encoded PUMA_2901.\n",
      "Encoded PUMA_2902.\n",
      "Encoded PUMA_2903.\n",
      "Encoded PUMA_2904.\n",
      "Encoded PUMA_2905.\n",
      "Encoded PUMA_3100.\n",
      "Encoded PUMA_3300.\n",
      "Encoded PUMA_3701.\n",
      "Encoded PUMA_3702.\n",
      "Encoded PUMA_3703.\n",
      "Encoded PUMA_3704.\n",
      "Encoded PUMA_3705.\n",
      "Encoded PUMA_3706.\n",
      "Encoded PUMA_3707.\n",
      "Encoded PUMA_3708.\n",
      "Encoded PUMA_3709.\n",
      "Encoded PUMA_3710.\n",
      "Encoded PUMA_3711.\n",
      "Encoded PUMA_3712.\n",
      "Encoded PUMA_3713.\n",
      "Encoded PUMA_3714.\n",
      "Encoded PUMA_3715.\n",
      "Encoded PUMA_3716.\n",
      "Encoded PUMA_3717.\n",
      "Encoded PUMA_3718.\n",
      "Encoded PUMA_3719.\n",
      "Encoded PUMA_3720.\n",
      "Encoded PUMA_3721.\n",
      "Encoded PUMA_3722.\n",
      "Encoded PUMA_3723.\n",
      "Encoded PUMA_3724.\n",
      "Encoded PUMA_3725.\n",
      "Encoded PUMA_3726.\n",
      "Encoded PUMA_3727.\n",
      "Encoded PUMA_3728.\n",
      "Encoded PUMA_3729.\n",
      "Encoded PUMA_3730.\n",
      "Encoded PUMA_3731.\n",
      "Encoded PUMA_3732.\n",
      "Encoded PUMA_3733.\n",
      "Encoded PUMA_3734.\n",
      "Encoded PUMA_3735.\n",
      "Encoded PUMA_3736.\n",
      "Encoded PUMA_3737.\n",
      "Encoded PUMA_3738.\n",
      "Encoded PUMA_3739.\n",
      "Encoded PUMA_3740.\n",
      "Encoded PUMA_3741.\n",
      "Encoded PUMA_3742.\n",
      "Encoded PUMA_3743.\n",
      "Encoded PUMA_3744.\n",
      "Encoded PUMA_3745.\n",
      "Encoded PUMA_3746.\n",
      "Encoded PUMA_3747.\n",
      "Encoded PUMA_3748.\n",
      "Encoded PUMA_3749.\n",
      "Encoded PUMA_3750.\n",
      "Encoded PUMA_3751.\n",
      "Encoded PUMA_3752.\n",
      "Encoded PUMA_3753.\n",
      "Encoded PUMA_3754.\n",
      "Encoded PUMA_3755.\n",
      "Encoded PUMA_3756.\n",
      "Encoded PUMA_3757.\n",
      "Encoded PUMA_3758.\n",
      "Encoded PUMA_3759.\n",
      "Encoded PUMA_3760.\n",
      "Encoded PUMA_3761.\n",
      "Encoded PUMA_3762.\n",
      "Encoded PUMA_3763.\n",
      "Encoded PUMA_3764.\n",
      "Encoded PUMA_3765.\n",
      "Encoded PUMA_3766.\n",
      "Encoded PUMA_3767.\n",
      "Encoded PUMA_3768.\n",
      "Encoded PUMA_3769.\n",
      "Encoded PUMA_3900.\n",
      "Encoded PUMA_4101.\n",
      "Encoded PUMA_4102.\n",
      "Encoded PUMA_4701.\n",
      "Encoded PUMA_4702.\n",
      "Encoded PUMA_5301.\n",
      "Encoded PUMA_5302.\n",
      "Encoded PUMA_5303.\n",
      "Encoded PUMA_5500.\n",
      "Encoded PUMA_5700.\n",
      "Encoded PUMA_5901.\n",
      "Encoded PUMA_5902.\n",
      "Encoded PUMA_5903.\n",
      "Encoded PUMA_5904.\n",
      "Encoded PUMA_5905.\n",
      "Encoded PUMA_5906.\n",
      "Encoded PUMA_5907.\n",
      "Encoded PUMA_5908.\n",
      "Encoded PUMA_5909.\n",
      "Encoded PUMA_5910.\n",
      "Encoded PUMA_5911.\n",
      "Encoded PUMA_5912.\n",
      "Encoded PUMA_5913.\n",
      "Encoded PUMA_5914.\n",
      "Encoded PUMA_5915.\n",
      "Encoded PUMA_5916.\n",
      "Encoded PUMA_5917.\n",
      "Encoded PUMA_5918.\n",
      "Encoded PUMA_6101.\n",
      "Encoded PUMA_6102.\n",
      "Encoded PUMA_6103.\n",
      "Encoded PUMA_6501.\n",
      "Encoded PUMA_6502.\n",
      "Encoded PUMA_6503.\n",
      "Encoded PUMA_6504.\n",
      "Encoded PUMA_6505.\n",
      "Encoded PUMA_6506.\n",
      "Encoded PUMA_6507.\n",
      "Encoded PUMA_6508.\n",
      "Encoded PUMA_6509.\n",
      "Encoded PUMA_6510.\n",
      "Encoded PUMA_6511.\n",
      "Encoded PUMA_6512.\n",
      "Encoded PUMA_6513.\n",
      "Encoded PUMA_6514.\n",
      "Encoded PUMA_6515.\n",
      "Encoded PUMA_6701.\n",
      "Encoded PUMA_6702.\n",
      "Encoded PUMA_6703.\n",
      "Encoded PUMA_6704.\n",
      "Encoded PUMA_6705.\n",
      "Encoded PUMA_6706.\n",
      "Encoded PUMA_6707.\n",
      "Encoded PUMA_6708.\n",
      "Encoded PUMA_6709.\n",
      "Encoded PUMA_6710.\n",
      "Encoded PUMA_6711.\n",
      "Encoded PUMA_6712.\n",
      "Encoded PUMA_7101.\n",
      "Encoded PUMA_7102.\n",
      "Encoded PUMA_7103.\n",
      "Encoded PUMA_7104.\n",
      "Encoded PUMA_7105.\n",
      "Encoded PUMA_7106.\n",
      "Encoded PUMA_7107.\n",
      "Encoded PUMA_7108.\n",
      "Encoded PUMA_7109.\n",
      "Encoded PUMA_7110.\n",
      "Encoded PUMA_7111.\n",
      "Encoded PUMA_7112.\n",
      "Encoded PUMA_7113.\n",
      "Encoded PUMA_7114.\n",
      "Encoded PUMA_7115.\n",
      "Encoded PUMA_7301.\n",
      "Encoded PUMA_7302.\n",
      "Encoded PUMA_7303.\n",
      "Encoded PUMA_7304.\n",
      "Encoded PUMA_7305.\n",
      "Encoded PUMA_7306.\n",
      "Encoded PUMA_7307.\n",
      "Encoded PUMA_7308.\n",
      "Encoded PUMA_7309.\n",
      "Encoded PUMA_7310.\n",
      "Encoded PUMA_7311.\n",
      "Encoded PUMA_7312.\n",
      "Encoded PUMA_7313.\n",
      "Encoded PUMA_7314.\n",
      "Encoded PUMA_7315.\n",
      "Encoded PUMA_7316.\n",
      "Encoded PUMA_7317.\n",
      "Encoded PUMA_7318.\n",
      "Encoded PUMA_7319.\n",
      "Encoded PUMA_7320.\n",
      "Encoded PUMA_7321.\n",
      "Encoded PUMA_7322.\n",
      "Encoded PUMA_7501.\n",
      "Encoded PUMA_7502.\n",
      "Encoded PUMA_7503.\n",
      "Encoded PUMA_7504.\n",
      "Encoded PUMA_7505.\n",
      "Encoded PUMA_7506.\n",
      "Encoded PUMA_7507.\n",
      "Encoded PUMA_7701.\n",
      "Encoded PUMA_7702.\n",
      "Encoded PUMA_7703.\n",
      "Encoded PUMA_7704.\n",
      "Encoded PUMA_7901.\n",
      "Encoded PUMA_7902.\n",
      "Encoded PUMA_8101.\n",
      "Encoded PUMA_8102.\n",
      "Encoded PUMA_8103.\n",
      "Encoded PUMA_8104.\n",
      "Encoded PUMA_8105.\n",
      "Encoded PUMA_8106.\n",
      "Encoded PUMA_8301.\n",
      "Encoded PUMA_8302.\n",
      "Encoded PUMA_8303.\n",
      "Encoded PUMA_8501.\n",
      "Encoded PUMA_8502.\n",
      "Encoded PUMA_8503.\n",
      "Encoded PUMA_8504.\n",
      "Encoded PUMA_8505.\n",
      "Encoded PUMA_8506.\n",
      "Encoded PUMA_8507.\n",
      "Encoded PUMA_8508.\n",
      "Encoded PUMA_8509.\n",
      "Encoded PUMA_8510.\n",
      "Encoded PUMA_8511.\n",
      "Encoded PUMA_8512.\n",
      "Encoded PUMA_8513.\n",
      "Encoded PUMA_8514.\n",
      "Encoded PUMA_8701.\n",
      "Encoded PUMA_8702.\n",
      "Encoded PUMA_8900.\n",
      "Encoded PUMA_9501.\n",
      "Encoded PUMA_9502.\n",
      "Encoded PUMA_9503.\n",
      "Encoded PUMA_9701.\n",
      "Encoded PUMA_9702.\n",
      "Encoded PUMA_9703.\n",
      "Encoded PUMA_9901.\n",
      "Encoded PUMA_9902.\n",
      "Encoded PUMA_9903.\n",
      "Encoded PUMA_9904.\n",
      "Encoded PUMA_10100.\n",
      "Encoded PUMA_10701.\n",
      "Encoded PUMA_10702.\n",
      "Encoded PUMA_10703.\n",
      "Encoded PUMA_11101.\n",
      "Encoded PUMA_11102.\n",
      "Encoded PUMA_11103.\n",
      "Encoded PUMA_11104.\n",
      "Encoded PUMA_11105.\n",
      "Encoded PUMA_11106.\n",
      "Encoded PUMA_11300.\n",
      "Encoded ADJINC_1013097.\n",
      "Encoded WIF_0.\n",
      "Encoded WIF_1.0.\n",
      "Encoded WIF_2.0.\n",
      "Encoded WIF_3.0.\n",
      "Encoded WORKSTAT_0.\n",
      "Encoded WORKSTAT_1.0.\n",
      "Encoded WORKSTAT_2.0.\n",
      "Encoded WORKSTAT_3.0.\n",
      "Encoded WORKSTAT_4.0.\n",
      "Encoded WORKSTAT_5.0.\n",
      "Encoded WORKSTAT_6.0.\n",
      "Encoded WORKSTAT_7.0.\n",
      "Encoded WORKSTAT_8.0.\n",
      "Encoded WORKSTAT_9.0.\n",
      "Encoded WORKSTAT_10.0.\n",
      "Encoded WORKSTAT_11.0.\n",
      "Encoded WORKSTAT_12.0.\n",
      "Encoded WORKSTAT_13.0.\n",
      "Encoded WORKSTAT_14.0.\n",
      "Encoded WORKSTAT_15.0.\n",
      "Encoded HHL_1.0.\n",
      "Encoded HHL_2.0.\n",
      "Encoded HHL_3.0.\n",
      "Encoded HHL_4.0.\n",
      "Encoded HHL_5.0.\n",
      "Encoded FPARC_0.\n",
      "Encoded FPARC_1.0.\n",
      "Encoded FPARC_2.0.\n",
      "Encoded FPARC_3.0.\n",
      "Encoded FPARC_4.0.\n",
      "Encoded HHT_1.0.\n",
      "Encoded HHT_2.0.\n",
      "Encoded HHT_3.0.\n",
      "Encoded HHT_4.0.\n",
      "Encoded HHT_5.0.\n",
      "Encoded HHT_6.0.\n",
      "Encoded HHT_7.0.\n",
      "Encoded HUGCL_1.0.\n",
      "Encoded HUGCL_2.0.\n",
      "Encoded HUPAC_1.0.\n",
      "Encoded HUPAC_2.0.\n",
      "Encoded HUPAC_3.0.\n",
      "Encoded HUPAC_4.0.\n",
      "Encoded LNGI_1.0.\n",
      "Encoded LNGI_2.0.\n",
      "Encoded MULTG_1.0.\n",
      "Encoded MULTG_2.0.\n",
      "Encoded NPP_1.0.\n",
      "Encoded NPP_2.0.\n",
      "Encoded NR_1.0.\n",
      "Encoded NR_2.0.\n",
      "Encoded PARTNER_0.0.\n",
      "Encoded PARTNER_1.0.\n",
      "Encoded PARTNER_2.0.\n",
      "Encoded PARTNER_3.0.\n",
      "Encoded PARTNER_4.0.\n",
      "Encoded PSF_1.0.\n",
      "Encoded PSF_2.0.\n",
      "Encoded R18_1.0.\n",
      "Encoded R18_2.0.\n",
      "Encoded R65_0.0.\n",
      "Encoded R65_1.0.\n",
      "Encoded R65_2.0.\n",
      "Encoded SSMC_0.0.\n",
      "Encoded SSMC_1.0.\n",
      "Encoded SSMC_2.0.\n",
      "Encoded ACCESS_1.0.\n",
      "Encoded ACCESS_2.0.\n",
      "Encoded ACCESS_3.0.\n",
      "Encoded BROADBND_0.\n",
      "Encoded BROADBND_1.0.\n",
      "Encoded BROADBND_2.0.\n",
      "Encoded COMPOTHX_1.0.\n",
      "Encoded COMPOTHX_2.0.\n",
      "Encoded DIALUP_0.\n",
      "Encoded DIALUP_1.0.\n",
      "Encoded DIALUP_2.0.\n",
      "Encoded HISPEED_0.\n",
      "Encoded HISPEED_1.0.\n",
      "Encoded HISPEED_2.0.\n",
      "Encoded LAPTOP_1.0.\n",
      "Encoded LAPTOP_2.0.\n",
      "Encoded OTHSVCEX_0.\n",
      "Encoded OTHSVCEX_1.0.\n",
      "Encoded OTHSVCEX_2.0.\n",
      "Encoded SATELLITE_0.\n",
      "Encoded SATELLITE_1.0.\n",
      "Encoded SATELLITE_2.0.\n",
      "Encoded SMARTPHONE_1.0.\n",
      "Encoded SMARTPHONE_2.0.\n",
      "Encoded TABLET_1.0.\n",
      "Encoded TABLET_2.0.\n",
      "Encoded TEL_1.0.\n",
      "Encoded TEL_2.0.\n",
      "Encoded TYPE_1.\n",
      "Encoded BATH_1.0.\n",
      "Encoded BATH_2.0.\n",
      "Encoded REFR_1.0.\n",
      "Encoded REFR_2.0.\n",
      "Encoded RWAT_1.0.\n",
      "Encoded RWAT_2.0.\n",
      "Encoded SINK_1.0.\n",
      "Encoded SINK_2.0.\n",
      "Encoded STOV_1.0.\n",
      "Encoded STOV_2.0.\n",
      "Encoded TEN_1.0.\n",
      "Encoded TEN_2.0.\n",
      "Encoded TEN_3.0.\n",
      "Encoded TEN_4.0.\n",
      "Encoded KIT_1.0.\n",
      "Encoded KIT_2.0.\n",
      "Encoded MV_1.0.\n",
      "Encoded MV_2.0.\n",
      "Encoded MV_3.0.\n",
      "Encoded MV_4.0.\n",
      "Encoded MV_5.0.\n",
      "Encoded MV_6.0.\n",
      "Encoded MV_7.0.\n",
      "Encoded PLM_1.0.\n",
      "Encoded PLM_2.0.\n",
      "Encoded SRNT_1.0.\n",
      "Encoded SRNT_2.0.\n",
      "Encoded SVAL_1.0.\n",
      "Encoded SVAL_2.0.\n",
      "Encoded ELEFP_1.0.\n",
      "Encoded ELEFP_2.0.\n",
      "Encoded ELEFP_3.0.\n",
      "Encoded FULFP_1.0.\n",
      "Encoded FULFP_2.0.\n",
      "Encoded FULFP_3.0.\n",
      "Encoded GASFP_0.\n",
      "Encoded GASFP_1.0.\n",
      "Encoded GASFP_2.0.\n",
      "Encoded GASFP_3.0.\n",
      "Encoded HFL_1.0.\n",
      "Encoded HFL_2.0.\n",
      "Encoded HFL_3.0.\n",
      "Encoded HFL_4.0.\n",
      "Encoded HFL_5.0.\n",
      "Encoded HFL_6.0.\n",
      "Encoded HFL_7.0.\n",
      "Encoded HFL_8.0.\n",
      "Encoded HFL_9.0.\n",
      "Encoded RNTM_0.\n",
      "Encoded RNTM_1.0.\n",
      "Encoded RNTM_2.0.\n",
      "Encoded WATFP_1.0.\n",
      "Encoded WATFP_2.0.\n",
      "Encoded WATFP_3.0.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Function to transform categorical variables\n",
    "    \n",
    "def encoder_transform(encoder, X):\n",
    "    X_encoded = encoder.transform(X).toarray()\n",
    "    encoded_feats = list(encoder.get_feature_names())\n",
    "    feats = X.columns\n",
    "    encoded_feats_updated = []\n",
    "    for feat in encoded_feats:\n",
    "        feat_split = feat.split('_')\n",
    "        i = int(feat_split[0][1:])\n",
    "        dummies = feat_split[1]\n",
    "        feat_updated = f'{feats[i]}_{dummies}'\n",
    "        encoded_feats_updated.append(feat_updated)\n",
    "        print(f'Encoded {feat_updated}.')\n",
    "    return pd.DataFrame(X_encoded, columns = encoded_feats_updated)\n",
    "        \n",
    "y = df.FS\n",
    "X = df.drop(['FS', 'SERIALNO', 'REGION', 'DIVISION', 'ST', 'HOTWAT', 'PLMPRP', 'RWATPR'], axis = 1)\n",
    "# Split the data into test and training samples--stratify by SNAP recipiency\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                    stratify = y, \\\n",
    "                                                    test_size = 0.2, \\\n",
    "                                                    random_state = 1007)\n",
    "print(\"Completed test train split.\")\n",
    "# Scaling and one-hot-encoding of the training set\n",
    "cont_feats = ['HINCP', 'VEH', 'NP', 'NPF', 'NRC', 'BDSP', 'BLD', 'RMSP', \\\n",
    "              'YBL', 'CONP', 'ELEP', 'GASP', 'FULP', 'INSP', 'MHP', \\\n",
    "              'RNTP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "sclr = StandardScaler()\n",
    "X_train_cont = pd.DataFrame(sclr.fit_transform(X_train[cont_feats]), \\\n",
    "                            columns = cont_feats)\n",
    "print(\"Finished scaling.\")\n",
    "cat_feats = X_train.drop(cont_feats, axis = 1).columns\n",
    "encdr = OneHotEncoder(handle_unknown = 'ignore')\n",
    "encdr.fit(X_train[cat_feats])\n",
    "X_train_cat = encoder_transform(encdr, X_train[cat_feats])\n",
    "X_train = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "\n",
    "# Scaling and one-hot-encoding of the test set\n",
    "X_test_cont = pd.DataFrame(sclr.transform(X_test[cont_feats]), \\\n",
    "                           columns = cont_feats)\n",
    "X_test_cat = encoder_transform(encdr, X_test[cat_feats])\n",
    "X_test = pd.concat((X_test_cont, X_test_cat), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HINCP</th>\n",
       "      <th>VEH</th>\n",
       "      <th>NP</th>\n",
       "      <th>NPF</th>\n",
       "      <th>NRC</th>\n",
       "      <th>BDSP</th>\n",
       "      <th>BLD</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>YBL</th>\n",
       "      <th>CONP</th>\n",
       "      <th>...</th>\n",
       "      <th>HFL_6.0</th>\n",
       "      <th>HFL_7.0</th>\n",
       "      <th>HFL_8.0</th>\n",
       "      <th>HFL_9.0</th>\n",
       "      <th>RNTM_0</th>\n",
       "      <th>RNTM_1.0</th>\n",
       "      <th>RNTM_2.0</th>\n",
       "      <th>WATFP_1.0</th>\n",
       "      <th>WATFP_2.0</th>\n",
       "      <th>WATFP_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121290</td>\n",
       "      <td>-1.709006</td>\n",
       "      <td>-1.071423</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-2.290219</td>\n",
       "      <td>1.549852</td>\n",
       "      <td>-1.975683</td>\n",
       "      <td>-0.673670</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620209</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.824870</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>0.267488</td>\n",
       "      <td>-1.089291</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170364</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.824870</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>1.442996</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>2.031653</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>-1.071423</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>2.404761</td>\n",
       "      <td>-0.202898</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.252154</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>1.456967</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>3.447962</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HINCP       VEH        NP       NPF       NRC      BDSP       BLD  \\\n",
       "0 -0.121290 -1.709006 -1.071423 -1.169563 -0.561970 -2.290219  1.549852   \n",
       "1 -0.620209  0.006660  0.824870  0.944122  0.440513 -1.434189  0.267488   \n",
       "2 -0.170364  0.864493  0.824870  0.944122  1.442996  0.277871 -0.587421   \n",
       "3  0.001396 -0.851173 -1.071423 -1.169563 -0.561970 -1.434189  2.404761   \n",
       "4 -0.252154 -0.851173  1.456967  1.472543  3.447962  0.277871 -0.587421   \n",
       "\n",
       "       RMSP       YBL      CONP  ...  HFL_6.0  HFL_7.0  HFL_8.0  HFL_9.0  \\\n",
       "0 -1.975683 -0.673670 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "1 -1.089291 -0.072487 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "2  0.240298  2.031653 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "3 -0.202898 -0.072487 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "4  0.240298  0.228104 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   RNTM_0  RNTM_1.0  RNTM_2.0  WATFP_1.0  WATFP_2.0  WATFP_3.0  \n",
       "0     0.0       1.0       0.0        0.0        1.0        0.0  \n",
       "1     0.0       1.0       0.0        1.0        0.0        0.0  \n",
       "2     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "3     0.0       1.0       0.0        0.0        0.0        1.0  \n",
       "4     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the calculate_vif code, but it continues to hang and I can't see why.  Beneath the following cell, I start running vif individually and dropping the max column it returns by hand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X, thresh=100):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        print('Entered while loop')\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        print('Created c')\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        print('Calculated vif')\n",
    "        maxloc = vif.index(max(vif))\n",
    "        print(f'Max = {maxloc} at {max(vif)}')\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X[cols[variables]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual vif and column dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 17 at 183.38035314331293\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.275597\n",
       "1         0.318254\n",
       "2        -0.662864\n",
       "3         0.867467\n",
       "4        -0.662864\n",
       "            ...   \n",
       "107418    1.664092\n",
       "107419   -0.662864\n",
       "107420   -0.023004\n",
       "107421   -0.662864\n",
       "107422   -0.662864\n",
       "Name: GRNTP, Length: 107423, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['GRNTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 2 at 7.861800044133322\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -1.071423\n",
       "1         0.824870\n",
       "2         0.824870\n",
       "3        -1.071423\n",
       "4         1.456967\n",
       "            ...   \n",
       "107418   -1.071423\n",
       "107419   -0.439326\n",
       "107420    2.089065\n",
       "107421    0.824870\n",
       "107422    0.824870\n",
       "Name: NP, Length: 107423, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 4 at 3.5718768321254966\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After identifying the columns with collinearity issues, I drop them from X_test as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns = ['NP', 'GRNTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HINCP</th>\n",
       "      <th>VEH</th>\n",
       "      <th>NPF</th>\n",
       "      <th>NRC</th>\n",
       "      <th>BDSP</th>\n",
       "      <th>BLD</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>YBL</th>\n",
       "      <th>CONP</th>\n",
       "      <th>ELEP</th>\n",
       "      <th>...</th>\n",
       "      <th>HFL_6.0</th>\n",
       "      <th>HFL_7.0</th>\n",
       "      <th>HFL_8.0</th>\n",
       "      <th>HFL_9.0</th>\n",
       "      <th>RNTM_0</th>\n",
       "      <th>RNTM_1.0</th>\n",
       "      <th>RNTM_2.0</th>\n",
       "      <th>WATFP_1.0</th>\n",
       "      <th>WATFP_2.0</th>\n",
       "      <th>WATFP_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121290</td>\n",
       "      <td>-1.709006</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-2.290219</td>\n",
       "      <td>1.549852</td>\n",
       "      <td>-1.975683</td>\n",
       "      <td>-0.673670</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-1.148211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620209</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>0.267488</td>\n",
       "      <td>-1.089291</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170364</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>1.442996</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>2.031653</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>2.404761</td>\n",
       "      <td>-0.202898</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.503327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.252154</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>3.447962</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.226948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HINCP       VEH       NPF       NRC      BDSP       BLD      RMSP  \\\n",
       "0 -0.121290 -1.709006 -1.169563 -0.561970 -2.290219  1.549852 -1.975683   \n",
       "1 -0.620209  0.006660  0.944122  0.440513 -1.434189  0.267488 -1.089291   \n",
       "2 -0.170364  0.864493  0.944122  1.442996  0.277871 -0.587421  0.240298   \n",
       "3  0.001396 -0.851173 -1.169563 -0.561970 -1.434189  2.404761 -0.202898   \n",
       "4 -0.252154 -0.851173  1.472543  3.447962  0.277871 -0.587421  0.240298   \n",
       "\n",
       "        YBL      CONP      ELEP  ...  HFL_6.0  HFL_7.0  HFL_8.0  HFL_9.0  \\\n",
       "0 -0.673670 -0.213087 -1.148211  ...      0.0      0.0      0.0      0.0   \n",
       "1 -0.072487 -0.213087 -0.687579  ...      0.0      0.0      0.0      0.0   \n",
       "2  2.031653 -0.213087 -0.687579  ...      0.0      0.0      0.0      0.0   \n",
       "3 -0.072487 -0.213087 -0.503327  ...      0.0      0.0      0.0      0.0   \n",
       "4  0.228104 -0.213087 -0.226948  ...      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   RNTM_0  RNTM_1.0  RNTM_2.0  WATFP_1.0  WATFP_2.0  WATFP_3.0  \n",
       "0     0.0       1.0       0.0        0.0        1.0        0.0  \n",
       "1     0.0       1.0       0.0        1.0        0.0        0.0  \n",
       "2     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "3     0.0       1.0       0.0        0.0        0.0        1.0  \n",
       "4     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "\n",
       "[5 rows x 432 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running vanilla LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype('int')\n",
    "clf = LogisticRegression(random_state=1007, solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.99      0.96     24819\n",
      "           2       0.60      0.19      0.29      2037\n",
      "\n",
      "    accuracy                           0.93     26856\n",
      "   macro avg       0.77      0.59      0.62     26856\n",
      "weighted avg       0.91      0.93      0.91     26856\n",
      "\n",
      "[[24564   255]\n",
      " [ 1655   382]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_test = y_test.astype('int')\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not terrific.  Let's try a deep GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty':('l1', 'l2', 'none'), 'C':[0.001, 0.01, 0.1, 1, 10, 100, 1e9]}\n",
    "clfgs = GridSearchCV(clf, parameters, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=1007, solver='saga',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000000000.0],\n",
       "                         'penalty': ('l1', 'l2', 'none')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.99      0.96     24819\n",
      "           2       0.60      0.19      0.29      2037\n",
      "\n",
      "    accuracy                           0.93     26856\n",
      "   macro avg       0.77      0.59      0.62     26856\n",
      "weighted avg       0.91      0.93      0.91     26856\n",
      "\n",
      "[[24564   255]\n",
      " [ 1655   382]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXACTLY the same.  Not great.  The classes are pretty imbalanced though (around 12:1), so SMOTE may be a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 1)/len(y_train) # shows classes are evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another vanilla Logreg - the same parameters (C = 1, penalty = l2) came out in GridSearchCV so we'll stick with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=1007, solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.79      0.87     24819\n",
      "           2       0.23      0.77      0.35      2037\n",
      "\n",
      "    accuracy                           0.79     26856\n",
      "   macro avg       0.60      0.78      0.61     26856\n",
      "weighted avg       0.92      0.79      0.83     26856\n",
      "\n",
      "[[19540  5279]\n",
      " [  474  1563]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now precision is low but recall is much better.  Let's try to visualize the precision-recall curve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: f1=0.872 auc=0.387\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deZhU5Zn38e/NThRBlhixGxsnxMhgi9p2IMRMJojbEJxE40IMYlQmvhIxo2SIcbB1Mo4mxi2aIShqNIm7ibhHJcYlIDaDiKAotCAtREGRGAgCzf3+caqa6q7q7lPddWrp8/tcV11VZ78PS931LOd5zN0REZH46lLoAEREpLCUCEREYk6JQEQk5pQIRERiTolARCTmuhU6gGwNHDjQKyoqCh2GiEhJWbRo0UZ3H5RpW8klgoqKCmprawsdhohISTGzNS1tU9WQiEjMKRGIiMScEoGISMwpEYiIxJwSgYhIzEWWCMzsVjN738xea2G7mdkNZrbSzF41s8OiikVERFoWZYngduDYVrYfBwxLvKYA/xthLLB2ITz/s+BdREQaRfYcgbs/Z2YVrexyAnCHB+NgLzCzfma2r7uvz3kwaxfC7eOhYTt06wlnPAzl1Tm/jIhIKSpkG8F+wNqU5frEujRmNsXMas2sdsOGDdlfafXzQRLAoWFHsCwiIkBhE4FlWJdxlhx3n+3uVe5eNWhQxiekW1dxJHTpGnzu2j1YFhERoLCJoB4oT1kuA9ZFcqXyajh8cvB54r2qFhIRSVHIRDAXmJToPTQK2BxJ+0BSvyHBe1lVZJcQESlFkTUWm9ldwFeAgWZWD1wKdAdw91nAY8DxwEpgK3BmVLGIiEjLouw1dFob2x04L6rri4hIOHqyWEQk5pQIRERiTolARCTmlAhERGJOiUBEJOaUCEREYk6JQEQk5pQIRERiLrIHyorVpDkL+aRLr8bl8ZX78u3RFfx9ewOTb0ufq+Ckw8v4ZlU5H27Zzrm/XpS2/fRR+/O1Qwaz7qO/8/17Xknbfs6RB3DU8H1YteFvXPzg0rTt3/vqML40bCDL1m3m8oeXp23/wbEHcvj+/Vm05kN+8sSKtO0zvzacfxzclxfe2sjP572Vtv2KbxzMPwzak6eXv8fNz9elbb/2lJEM7tebh5es49cL1qRt/9/TD6f/Hj24r3Yt9y+qT9t++5nV9O7RlTvnr+aRV9NHCLnn30YDMPu5VTzz+vtNtvXq3pVffScY9+mGZ97ixZUbm2zf+1M9mPXtwwG46ok3+L81m5ps37dvL6479VAALnt4GcvX/bXJ9gMG7cH/fKMSgB8++Cp1G7Y02T588F5c+rV/BOCCuxezfvO2JtsP239v/uPYzwPw3TsXsWnr9ibbx3x2IOePHQbAGbcuZNuOhibbxx70aaZ8+R8AOOWX82lO//b0bw+y+7eXvKdcU4lARCTmLBjpoXRUVVV5bW1t9ge+eD08NRMuXgc99sh9YCIiRczMFrl7xlE3VSIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZiL3cQ0TaxdCEvuAhwOmQjl1YWOSEQk7+KbCNYuhDnHALuC5cW/gcmPKhmISOzEt2roD/9JYxIAaNgOq58vWDgiIoUSz0SwdiGsXdBspUHFkQUJR0SkkOKXCOpr4aHz0tf3H6pqIRGJpfglgt+eDBvfTF//4eqgpCAiEjORJgIzO9bMVpjZSjObkWH7EDP7o5ktNrNXzez4KOMBYOe2FjbsUhuBiMRSZInAzLoCNwHHAcOB08xseLPdLgHudfdDgVOBX0QVTyi9BxT08iIihRBliaAaWOnude6+HbgbOKHZPg7slfjcF1gXYTxt+8srBb28iEghRJkI9gPWpizXJ9alqgFON7N64DHge5lOZGZTzKzWzGo3bNgQRayBv0V4bhGRIhVlIrAM67zZ8mnA7e5eBhwP3GlmaTG5+2x3r3L3qkGDBkUQqohIfEWZCOqB8pTlMtKrfs4C7gVw9/lAL2BghDG1bk8lGRGJnygTwcvAMDMbamY9CBqD5zbb5x1gLICZHUSQCKKpn/nonTZ2MOjZN5JLi4gUs8gSgbvvBKYCTwKvE/QOWmZml5vZhMRuFwLnmNkS4C5gsrs3rz7KjU2rW9iQrMFyePE6eOrSSC4vIlKsIh10zt0fI2gETl03M+XzcmBMlDE02ruihQ3N8s6L18Hn/0VPGYtIbMTnyeJ+Q8Lv+3RNZGGIiBSb+CSCTHr0ybz+L6/lNw4RkQKKbyLo0g16fCrztu698huLiEgBxScRpPUaMujWwhf+gM9GHo6ISLGITyJo3mto1074zMGZ9+29d+ThiIgUi/gkgua9hrp2hzHTYPz16Q3JbzyiIalFJDbikwhSv+z3GbF7fuKqyZmrgp6uCZLB8z9TUhCRTi0+k9enthFsWNF022cqYdW8puveWQBzxgWfu/aEyY/o2QIR6ZTiUyJIbSPY1dB0Eppee6Xtjjfs/tzwCbx4fWShiYgUUnwSQWobQZcuTSeqrzgS0gc9bUrtBiLSScUnETTRbITs8mrYq6ztw24/Hv5niMYjEpFOJT6JILVqyBvS5yfuHWLk0YYd8MlmDU4nIp1KfBqLU6uGrGvTqiGAnduzO9/8m4K2hYoj1YgsIiUtPokglWWYPG3gMNi4In19S3btgGcuhy7d4czH4L3l8PpDcNAJQZdUEZESEZ9EkKnXUOov+THTYMXjTXsLhbFrB9x2fPAOQTfUx38Aw0+AE2/ucNgiIlGLTxtBatVQ1x7pVUPl1fCdJ+Dz42HggcGcBGFnLEsmgaSGT2DpvTD7qx0KWUQkH+JTIkg+Wdxvfzjxlsz1+uXVcOpvdi+vXbj7obL2WLcIbqyGqep2KiLFKz4lgqR+Q8I37pZXw1lPwT4HJ+Yu6Jr99TauCJKBiEiRil8i2Fyf3YNh5dVw7gtwcT2c9UT7rrlxBdzx9fYdKyISsfgkguRYQ5vehl9NaN9TwskSwv5fzP7Yunlw23F6OllEik58EkFqr6GG7ekPlIVVXg1nPh4khKrvBHMa9BkMgw+HvQa3fuyaPwdtDkoGIlJE4tNY3FavoWyVV2dua1i7MBig7o1HWj72jn+FH63r2PVFRHIkPiWCZK+hQZ+HM+ZG9zRwsufRwSe3vM+OLdFcW0SkHeKTCJIGfT4/Q0KceDNYKwWumv6qIhKRohC/RNAlj7Vh33m8lY0Nai8QkaIQw0TQjmcB2itML6OOPLAmIpID8UsElsdEALt7GfXau+V9Lh+Yv3hERJqJXyLIZ4kg1YzVwdzHmezaoXGJRKRglAjy6T/fb7lksG4RXHtwfuMRESGOiSDfVUPNzVgdjG6ayeZ3VDIQkbyLNBGY2bFmtsLMVprZjBb2OdnMlpvZMjP7bZTxAPntNdSS1kYjXbcIavrCfw+GB86BGw7VtJgiEqnIEoGZdQVuAo4DhgOnmdnwZvsMA34IjHH3fwQuiCqeRutfKY4um3vs0/r2HVuCOQ0+rAvmSK7pC1dW5CU0EYmXKEsE1cBKd69z9+3A3cAJzfY5B7jJ3TcBuPv7kUWzKTHo3NqF7R90Lpemv9l2Mmhu2yYlAxHJudCJwMz2M7MvmtmXk682DtkPWJuyXJ9Yl+pzwOfM7EUzW2Bmx7Zw7SlmVmtmtRs2bAgbclOb6hIfvGODzuXS9DdhTJaFoG2bVDoQkZwKVWFuZlcBpwDLgeSkvg4819phGdZ5husPA74ClAHPm9kId/+oyUHus4HZAFVVVc3PEU6//XeHlYtB53Jl3GXw0mzYuTW745IJoe8Q+P7SaGITkVgIWyL4V+BAdz/e3b+WeE1o45h6oDxluQxoPuRmPfCQu+9w97eBFQSJIff6Jgoj+4+JdtC59rhkffbVREmb3wkSwgPn5DYmEYmNsF1o6oDuwCdZnPtlYJiZDQXeBU4FJjbb5/fAacDtZjaQoKqojigkyxFDRhVXEkia/mbm9bO+BH8J8Yt/6b3Ba/z1UDU5p6GJSOcWNhFsBV4xs2dISQbufn5LB7j7TjObCjxJMNnvre6+zMwuB2rdfW5i29Fmlqxymu7uH7TzXlrnu4J3y1RjVcT+5RqYczTptWoteGRa8Dr45GAEVJFOZMeOHdTX17Nt27ZCh1K0evXqRVlZGd27dw99TNhEMDfxyoq7PwY81mzdzJTPDvx74hWxxBepldgzdOXVcNYfYMld8Lf34d3/g49DTGqz9N7gXclAOpH6+nr69OlDRUUFVmo/6vLA3fnggw+or69n6NChoY8LlQjc/Vdm1oOg6gZghbvvaEechZMsEWRswy5ymWZD++nnYMt7rR+39F7YshEm/S662ETyaNu2bUoCrTAzBgwYQLa9K0P9PDazrwBvETwg9gvgzRDdR4uLl2iJoCXT3wyGuO7Wq/X96uYFjck3FmG7iEg7KAm0rj1/PmG/FX8GHO3u/+TuXwaOAa7N+mqFVKptBK0pr4ZL3gsaiNuycUWQEGr6wh1fjz42kU7KzLjwwgsbl6+++mpqampCH//ee+8xfvx4DjnkEIYPH87xxx8PwLPPPsv48ePT9p87dy5XXnklADU1NVx99dUATJ48mfvvv78Dd7Jb2ETQ3d1XJBfc/U2CXkQlJFki6ESJIKlqMtRsDt8FNVlK0BhGIlnr2bMnDz74IBs3bmzX8TNnzmTcuHEsWbKE5cuXN37Jt2TChAnMmJFxqLacCZsIas1sjpl9JfG6GVgUZWA5V8ptBGG11AW1JckxjFRCEAmtW7duTJkyhWuvTa8UWbNmDWPHjqWyspKxY8fyzjvvpO2zfv16ysrKGpcrKyvT9nn55Zc59NBDqaur4/bbb2fq1Km5vYlmwvYaOhc4Dzif4Jv0OYK2gtLR2doIWlKzGa4og+0fhz+mbl4w/PWUedHFJRKBU345P23d+Mp9+fboCv6+vYHJt6WPKXbS4WV8s6qcD7ds59xfN/09e8+/jQ513fPOO4/Kykp+8IMfNFk/depUJk2axBlnnMGtt97K+eefz+9///u0Y0855RRuvPFGjjrqKM4880wGDx7cuP3Pf/4z3/ve93jooYcYMmQIzz3X2gAOuRG219AnwDWJV2nqjG0ELbm4fvfnmn6EegYhOfx1t08FTzqLSIv22msvJk2axA033EDv3r0b18+fP58HH3wQgG9/+9tpiQLgmGOOoa6ujieeeILHH3+cQw89lNdeew2A119/nSlTpvCHP/yhSXKIWquJwMzudfeTzWwpGb5N3D29TFO0kuHHIBGkqvkoGH7i9YdgZ4gHw3duDRICKClI0WvtF3zvHl1b3d5/jx6hSwCZXHDBBRx22GGceeaZLe7TUg+e/v37M3HiRCZOnMj48eN57rnnGDBgAPvuuy/btm1j8eLFeU0EbdWTTEu8jwe+luFVOuJSNZTJiTfDJe8H1UY1m4Mv+DCSSUFTaIqk6d+/PyeffDJz5sxpXPfFL36Ru+++G4Df/OY3fOlLX0o7bt68eWzdGgwy+fHHH7Nq1SqGDBkCQL9+/Xj00Ue5+OKLefbZZ6O/iYRWvxXdPflzcCOw1t3XAD2BQ0gfQK64eSfuNZStS9bD4MPD758c2E5Emrjwwgub9B664YYbuO2226isrOTOO+/k+uvTu3YvWrSIqqoqKisrGT16NGeffTZHHHFE4/Z99tmHhx9+mPPOO4+XXnopL/dh7m3XH5vZIuBIYG9gAVALbHX3b0UbXrqqqiqvra3N/sAnfggLfgHHXAGjz8t9YKUo20ZlCOZPGHdZNPGItOH111/noIMOKnQYRS/Tn5OZLXL3qkz7h60nMXffCnwD+Lm7f51g+snSEYfuo9m6uB4O+Cp06UroPxd1ORXpdMJ2HzUzGw18Czgry2OLQ7LkU/cnKKsqzqGoCyHTOESX9QdvSF+fKvlQGgb7j4ajLtOfqUiJClsiuIBgkvnfJYaSPgD4Y3RhReDjRHPHW08Wx5zFxezSD8MNWwGAw5o/w5xxQQIRkZIT9jmCPwF/SlmuI3i4rHT8Ndm2nTJnsX7BtqxqcvD68b7hp9H0hvRG5R59mj7XICJFp9USgZldl3h/2MzmNn/lJ8Qc6fOZxIcim7O42F2yPhjltL1tK9s/Vo8jkSLXVongzsT71VEHErk9BgXvBx4HX/q+SgPZKK8OHkyDcO0HmSSTwf5fVHuCSJFp6zmC5EActcDz7v6nRDXRCwRzEpeOZK+hYeP0JdQRl34YPJQ28MD2HZ9sT0gOiZ18PXBObuMUiciee+7Z4XOsW7eOk046qcXtH330Eb/4xS9C799RYRuLnwFSH0ftDTyd+3CiFOMni6MwdeHuJ5WzeTitJUvvhf8erEZ8iYXBgwe3OpdA80TQ1v4dFfZbsZe7/y25kPgccpyCIqHnCKIzZV6QEM56CsbOTLQptMOOLbtLCxrWQnJl7UJ4/meR/shoafjpVatWMWrUKI444ghmzpzZWJpYvXo1I0aMAGDZsmVUV1czcuRIKisreeutt5gxYwarVq1i5MiRTJ8+vcn+DQ0NXHTRRRx88MFUVlby85//vMPxh30WYIuZHebu/wdgZocDf+/w1fOpccw5JYLIpM6tXLM5qO5Zem/7zpUc1qJrT/jP93MXo3Qej8+AvyxtfZ9P/grvvRb8ELQusM8I6LlXy/t/5mA4rvWJYjJpafjpadOmMW3aNE477TRmzZqV8dhZs2Yxbdo0vvWtb7F9+3YaGhq48soree2113jllVeAIHEkzZ49m7fffpvFixfTrVs3Pvzww6zjbS6b5wjuM7Pnzex54B4g2pkSck5VQ3l34s27Swrt/XNv+CS9PUEkrG2bd9cG+K5gOQLz589n4sSJQDD89AsvvNC4/pvf/CZA4/bmRo8ezRVXXMFVV13FmjVrmgxrncnTTz/Nd7/7Xbp1C37H9+/f8ed3wj5H8LKZfR44kKBu5Q1339Hhq+eTqoYKp7waLt2Uvn7twqAqKFs1fYNpObOdkU06lzC/3NcuDB4gbdgedBs/8Za8dBbJZgL5iRMn8oUvfIFHH32UY445hltuuYUDDjigxf3dvV0T1Lcm1M80M/sU8B/ANHdfClSYWfosy8UszsNQF6vy6qC00LMdv/K3vAc//VzuY5LOpbwazpgLX/1R8B5REmhp+OlRo0bxwAMPADRub66uro4DDjiA888/nwkTJvDqq6/Sp08fPv4484CQRx99NLNmzWLnzp0Aea0aug3YDiRncagHftzhq+dTnGYoKyXl1fDDd9rXJXXLe9HEJJ1LeTUceWHOksDWrVspKytrfF1zzTUtDj993XXXcc0111BdXc369evp2zf9R88999zDiBEjGDlyJG+88QaTJk1iwIABjBkzhhEjRjB9+vQm+5999tkMGTKEyspKDjnkEH772992+J7CDkNd6+5VZrbY3Q9NrFvi7od0OIIstXsY6gfOhqX3wTduhsqTcx+Y5NblA2FXyNrHmmjqfaX4lNow1Fu3bqV3796YGXfffTd33XUXDz30UOTXzXYY6rC9hrabWW8SLa5m9g9AiHkPi4jaCErLzI1Nl+/4ejDiaSY1fZUMpCgtWrSIqVOn4u7069ePW2+9tdAhZRQ2EVwKPAGUm9lvgDHA5KiCioRmKCttk37X+kQ6LfUm0kQ6UkBHHnkkS5YsKXQYbWozEVjQPP0GwaQ0owh+Uk9z942tHlhs1EZQ+i6uz7776IvXBS8InoCe0kKpQiTG2kwE7u5m9nt3Pxx4NA8xRUS9hjqFms3tf5Zg3aL0Y1WlVHKi6D7ZmYRp920u7LfiAjM7ou3dipjaCDqPXH551/SFGzUIYano1asXH3zwQbu+7OLA3fnggw/o1atXVseFbSP4Z+C7ZrYa2ELwberuXtnaQWZ2LHA90BW4xd0zPgFiZicB9wFHuHs7ugSFoOcIOpeaza03IGdj44qmJQWVEopWWVkZ9fX1bNiwodChFK1evXpRVlaW1TFhE8Fx2QZjZl2Bm4BxBM8dvGxmc919ebP9+hDMdvZSttfIihqLO59M8y1Dx8Y4gqZJQWMdFZXu3bszdOjQQofR6bSaCMysF/Bd4LPAUmCOu+8Mee5qYGViWkvM7G7gBGB5s/3+C/gJcFEWcbdD46hz0V5GCu/Em4MXtN7TKIzkWEdJ6oUknVBbJYJfATuA5wlKBcOBaSHPvR+wNmW5HvhC6g5mdihQ7u6PmFmLicDMpgBTAIYMGRLy8s2oaiieMs2XXLM3sCt9fRipvZC6fSqYylOkxLX1rTjc3U93918CJwHZTPSb6ad3YwuPmXUBrgUubOtE7j7b3avcvWrQoEFZhJB6EnUflYSaTblpB9i5FX68b8fPI1JgbZUIGp/xd/edWXbZqgfKU5bLgHUpy32AEcCzifN+BphrZhOiaTBWiUCaSU0G7a1C2rl1d9WRGpmlRLWVCA4xs78mPhvQO7Gc7DXUygwPvAwMM7OhwLvAqUDjgNzuvhkYmFw2s2eBi6LrNaTuo9KK5lVI2Yx1lKSeR1KiWk0E7t61vSdOlCCmAk8SdB+91d2XmdnlQK27z23vudsZUPCuEoGEkTrW0VOX7m4XCEulBCkhYbuPtou7PwY81mzdzBb2/UqUsexuI4j0KtIZjbtsd0+hKytgW4ZJdlqiAfGkBMTo57FKBJIDM1ZDr72zO0ZTbEqRi7REUFTURiC5MmP17s8/3jdoMA5D1UVSpGKUCFQikAikPkcQNinU9AW6BN1YRYpADBOBSgQSkWRSCFUNtCvzfnpyWQogRj+PVSKQPOlI1c+L1+1uU5j91dzFJNKK+Hwrbkv853z/jcLGIfFQszl4jbmg/edIzp9wx9dzF5dIBvFIBGsXwvuJse6evDhYFsmHcZd1vHG4bl6QEC4f2Pa+Iu0QjzaC1c/v7jW0a0ewXK7JSCSPksmgI91Id+3IfLx6IUkHxSMRVBwZtA34LujSPVgWKYSWvrQ7kiCSx3bp3vSJaJGQ4lE1VF4NAw8MPh93lUoDUnySbQoHdKCBOFli0MNrkqV4lAgAeu4ZvH96eGHjEGlN6qxruSglNC6r+khaFp9EkKTnCKRU5KJdofFcGhlVWhafRODe9j4ixSjTF/dl/cEb2nm+lhKLnnaOq/gkAs1ZLJ3JpR/u/pyzNoFdKjnEVDwai1Opakg6m2RDc87Pm2h4rr099+eWohKfEoGqhqSza54MavqRMk14+z0yLXiphNBpxScRNFKJQGKi5qPdnx84B5be28HzaRjtzipGiUAlAomxE28OXqlqb4dnr4C/vZfdudSO0OnEKBEkqEAgEqiaHLxSZdvwrKk4O4X4JAK1EYi0LfmlvnYhzBkX8phE8rCuTXszScmIUa8hdR8VCa28OkgKPfqEP8Ybdvc00jAXJSU+JYIkdR8VCe/i+uC9PY3NGuaiZMQnEahqSKT9Uhub2/trP3ncHvvA9DdzE5fkRIyqhpJUIhDpkI7+st/ynqqPikx8SgTqPiqSO6nJIBejpKraqKBilAgS1EYgklupX+KzvxrMtZz1OdSeUEjxSQQqEIhEb8q8pssdbU9oXFZiiFJ8EoG6j4rkX66rkJqfU3IiRokgQVVDIoWRq4l2lBRyLj6JQN1HRYpD8sv78oHBPMsdOleGpKLkkLVIE4GZHQtcD3QFbnH3K5tt/3fgbGAnsAH4jruviTImVQ2JFImZG5su56o7qXoiZS2yRGBmXYGbgHFAPfCymc119+Upuy0Gqtx9q5mdC/wEOCWaiFQiEClqafMp5LAKKdP5pVGUD5RVAyvdvc7dtwN3Ayek7uDuf3T3rYnFBUBZhPEE1EYgUhqSM6/l6gtcD7C1KMqqof2AtSnL9cAXWtn/LODxTBvMbAowBWDIkCHti0ZtBCKlK4reR83PG2NRJoJMP70zfhub2elAFfBPmba7+2xgNkBVVVU7v9HVfVSkU8j05d3R5xXGXADjLmt/TCUuyqqheqA8ZbkMWNd8JzM7CvgRMMHdP4kwnuQFI7+EiORZR6uQXrwu1lVHUZYIXgaGmdlQ4F3gVGBi6g5mdijwS+BYd38/wlhUNSQSBx1tcI7pMwqRlQjcfScwFXgSeB24192XmdnlZjYhsdtPgT2B+8zsFTObG1U8u6lEIBIbHfkyj9EkO5E+R+DujwGPNVs3M+XzUVFev1k0+buUiBSPXHRL7eQlhfjMR5CsGlIbgUi8JdsTBh7YzuM7XykhPkNMNFIiEBFg6sLg/coK2LYp++M7USkhRolAVUMiksGM1bs/52LY7BJMCjFKBAmqGhKRluTiwbUSTArxSQTqPioi2YhRUohPY3EjlQhEJEu5GPOoiBuZ41MiUBuBiHRULksJRVRCiE+JQN1HRSSXOjo6ahGVDmJUIhARiUh7SwpFUjqIUSJQ1ZCI5EF7kkKBE0J8qoaSVDUkIvmSbdVRgaqL4pMI1H1URAolm4RQgN5F8UkEjVQiEJECKdLSQYwSgUoEIlIEirB0EJ9E0DhTpUoEIlIEsk0IEYpPImikRCAiRaQIkkGMEoGqhkSkSIUtHUSUDGKUCBJUNSQixapAySA+iUDdR0WkFIQpHeQ4GcQnETRSiUBEJFWMEoFKBCJSQvI43ER8EoFGHxWRUtNSNVGOk0SMBp1LUiIQkRITcekgPiUCVQ2JiGQUo0SQoKohEZEm4pMI1H1URCSj+CSC3YMNFTQKEZFiE6NEkKCqIRGRJuKTCFQ1JCKSUXwSQSOVCEREUsUoEahEICKSSaSJwMyONbMVZrbSzGZk2N7TzO5JbH/JzCqijCdx0cgvISJSSiJLBGbWFbgJOA4YDpxmZsOb7XYWsMndPwtcC1wVVTxs3xq8L7knskuIiJSiKEsE1cBKd69z9+3A3cAJzfY5AfhV4vP9wFizCH6y194OO7YEn5+6JFgWEREg2kSwH7A2Zbk+sS7jPu6+E9gMDGh+IjObYma1Zla7YcOG7CN5/aHWl0VEYizKRJDpl33zFtsw++Dus929yt2rBg0alH0kB53Q+rKISIxFOfpoPVCeslwGrGthn3oz6wb0BT7MeSRVk4P317F6RB0AAAYNSURBVB8KkkByWUREIk0ELwPDzGwo8C5wKjCx2T5zgTOA+cBJwDz3iJ78qpqsBCAikkFkicDdd5rZVOBJoCtwq7svM7PLgVp3nwvMAe40s5UEJYFTo4pHREQyi3RiGnd/DHis2bqZKZ+3Ad+MMgYREWldjJ4sFhGRTJQIRERiTolARCTmlAhERGLOouqtGRUz2wCsaefhA4GNOQynFOie40H3HA8duef93T3jE7kllwg6wsxq3b2q0HHkk+45HnTP8RDVPatqSEQk5pQIRERiLm6JYHahAygA3XM86J7jIZJ7jlUbgYiIpItbiUBERJpRIhARiblOmQjM7FgzW2FmK81sRobtPc3snsT2l8ysIv9R5laIe/53M1tuZq+a2TNmtn8h4syltu45Zb+TzMzNrOS7Goa5ZzM7OfF3vczMfpvvGHMtxL/tIWb2RzNbnPj3fXwh4swVM7vVzN43s9da2G5mdkPiz+NVMzuswxd19071IhjyehVwANADWAIMb7bP/wNmJT6fCtxT6LjzcM//DHwq8fncONxzYr8+wHPAAqCq0HHn4e95GLAY2Dux/OlCx52He54NnJv4PBxYXei4O3jPXwYOA15rYfvxwOMEMzyOAl7q6DU7Y4mgGljp7nXuvh24G2g+N+UJwK8Sn+8HxppZpmkzS0Wb9+zuf3T3rYnFBQQzxpWyMH/PAP8F/ATYls/gIhLmns8BbnL3TQDu/n6eY8y1MPfswF6Jz31JnwmxpLj7c7Q+U+MJwB0eWAD0M7N9O3LNzpgI9gPWpizXJ9Zl3MfddwKbgQF5iS4aYe451VkEvyhKWZv3bGaHAuXu/kg+A4tQmL/nzwGfM7MXzWyBmR2bt+iiEeaea4DTzayeYP6T7+UntILJ9v97myKdmKZAMv2yb95HNsw+pST0/ZjZ6UAV8E+RRhS9Vu/ZzLoA1wKT8xVQHoT5e+5GUD30FYJS3/NmNsLdP4o4tqiEuefTgNvd/WdmNppg1sMR7r4r+vAKIuffX52xRFAPlKcsl5FeVGzcx8y6ERQnWyuKFbsw94yZHQX8CJjg7p/kKbaotHXPfYARwLNmtpqgLnVuiTcYh/23/ZC773D3t4EVBImhVIW557OAewHcfT7Qi2Bwts4q1P/3bHTGRPAyMMzMhppZD4LG4LnN9pkLnJH4fBIwzxOtMCWqzXtOVJP8kiAJlHq9MbRxz+6+2d0HunuFu1cQtItMcPfawoSbE2H+bf+eoGMAZjaQoKqoLq9R5laYe34HGAtgZgcRJIINeY0yv+YCkxK9h0YBm919fUdO2Omqhtx9p5lNBZ4k6HFwq7svM7PLgVp3nwvMISg+riQoCZxauIg7LuQ9/xTYE7gv0S7+jrtPKFjQHRTynjuVkPf8JHC0mS0HGoDp7v5B4aLumJD3fCFws5l9n6CKZHIp/7Azs7sIqvYGJto9LgW6A7j7LIJ2kOOBlcBW4MwOX7OE/7xERCQHOmPVkIiIZEGJQEQk5pQIRERiTolARCTmlAhERGJOiUCkGTNrMLNXzOw1M3vYzPrl+PyTzezGxOcaM7sol+cXyZYSgUi6v7v7SHcfQfCcyXmFDkgkSkoEIq2bT8qAXmY23cxeTowDf1nK+kmJdUvM7M7Euq8l5rtYbGZPm9k+BYhfpE2d7slikVwxs64EQxfMSSwfTTBuTzXBwF9zzezLwAcEYziNcfeNZtY/cYoXgFHu7mZ2NvADgqdgRYqKEoFIut5m9gpQASwCnkqsPzrxWpxY3pMgMRwC3O/uGwHcPTmAYRlwT2Ks+B7A23mJXiRLqhoSSfd3dx8J7E/wBZ5sIzDgfxLtByPd/bPuPiexPtNYLT8HbnT3g4F/IxgMTaToKBGItMDdNwPnAxeZWXeCgc++Y2Z7ApjZfmb2aeAZ4GQzG5BYn6wa6gu8m/h8BiJFSlVDIq1w98VmtgQ41d3vTAxzPD8xguvfgNMTo2H+N/AnM2sgqDqaTDBz1n1m9i7BMNhDC3EPIm3R6KMiIjGnqiERkZhTIhARiTklAhGRmFMiEBGJOSUCEZGYUyIQEYk5JQIRkZj7/9CATCMdCdfqAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict probabilities\n",
    "lr_probs = clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = clf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs, pos_label =2)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write a quick function that returns the 10 features with the highest overall influence (as calculated by absolute value of their betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUMA_3729',\n",
       " 'PUMA_1302',\n",
       " 'PUMA_8509',\n",
       " 'NP',\n",
       " 'HINCP',\n",
       " 'PUMA_6712',\n",
       " 'PUMA_8506',\n",
       " 'PUMA_3718',\n",
       " 'PUMA_3741',\n",
       " 'PUMA_10701']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = np.abs(clf.coef_[0])\n",
    "X_train_df = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "important_features = []\n",
    "for i in range(10):\n",
    "    index = np.where(betas == max(betas))[0][0]\n",
    "    feature = X_train_df.columns[index]\n",
    "    important_features.append(feature)\n",
    "    betas[index] = -1000\n",
    "\n",
    "important_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the betas with high positive correlation as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NP',\n",
       " 'PUMA_6712',\n",
       " 'PUMA_10701',\n",
       " 'PUMA_1907',\n",
       " 'WORKSTAT_3.0',\n",
       " 'PUMA_8503',\n",
       " 'PUMA_7507',\n",
       " 'PUMA_9703',\n",
       " 'PUMA_2902',\n",
       " 'PUMA_7101']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = clf.coef_[0]\n",
    "X_train_df = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "important_features = []\n",
    "for i in range(10):\n",
    "    index = np.where(betas == max(betas))[0][0]\n",
    "    feature = X_train_df.columns[index]\n",
    "    important_features.append(feature)\n",
    "    betas[index] = -1000\n",
    "\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of the 10 highest betas, only NP, PUMA_6712 and PUMA_10701 are positively correlated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
