{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function to drop weight columns\n",
    "def drop_weights(data_frame):\n",
    "    coltodrop = []\n",
    "    for i in range(1, 81):\n",
    "        coltodrop.append('WGTP' + str(i))\n",
    "    return data_frame.drop(coltodrop, axis = 1)\n",
    "\n",
    "# Function to drop allocation columns\n",
    "def drop_allocations(data_frame):\n",
    "    col = data_frame.columns\n",
    "    coltodrop = []\n",
    "    for c in col:\n",
    "        if (c[0]== 'F') & (c[-1] == 'P') & \\\n",
    "           (c != 'FULFP') & (c != 'FULP') & (c != 'FINCP'): \n",
    "            coltodrop.append(c)\n",
    "    return data_frame.drop(coltodrop, axis = 1)\n",
    "\n",
    "# Function to recode the categorical features \n",
    "def recode(data_frame):\n",
    "    # Binary categories\n",
    "    dict_bin = {0: 1, 1: 2}\n",
    "    bin_cols = ['HUGCL', 'NPP', 'NR', 'PSF', 'R18']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {1: 2, 2: 1}\n",
    "    bin_cols = ['BROADBND', 'COMPOTHX', 'DIALUP', 'HISPEED', 'LAPTOP', 'OTHSVCEX', \\\n",
    "                'SATELLITE', 'SMARTPHONE', 'TABLET', 'TEL', 'FS', 'BATH', 'REFR', \\\n",
    "                'SINK', 'STOV', 'KIT', 'RNTM']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {0: 2, 1: 1}\n",
    "    bin_cols = ['SRNT', 'SVAL']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    # Three categories\n",
    "    dict_bin = {1: 2, 2: 1, 9:0}\n",
    "    bin_cols = ['RWAT', 'RWATPR', 'PLM', 'PLMPRP', 'HOTWAT']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {1: 2, 2: 1, 3: 3}\n",
    "    bin_cols = ['ELEFP', 'FULFP', 'GASFP', 'WATFP']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    return data_frame\n",
    "    \n",
    "# Customized imputer function\n",
    "# Since fill_value is a constant, we apply the function to the entire dataset\n",
    "def imputer(data_frame):\n",
    "    imptr = SimpleImputer(strategy = 'constant', fill_value = 0)\n",
    "    relevant_cols = ['SERIALNO', 'REGION', 'DIVISION', 'ST', 'PUMA', 'FS', \\\n",
    "                     'HINCP', 'ADJINC', 'WIF', 'WORKSTAT', 'VEH', 'NP', 'HHL', \\\n",
    "                     'FPARC', 'HHT', 'HUGCL', 'HUPAC', 'LNGI', 'MULTG', 'NPF', \\\n",
    "                     'NPP', 'NR', 'NRC', 'PARTNER', 'PSF', 'R18', 'R65', 'SSMC', \\\n",
    "                     'ACCESS', 'BROADBND', 'COMPOTHX', 'DIALUP', 'HISPEED', 'LAPTOP', \\\n",
    "                     'OTHSVCEX', 'SATELLITE', 'SMARTPHONE', 'TABLET', 'TEL', \\\n",
    "                     'TYPE', 'BATH', 'BDSP', 'BLD', 'REFR', 'RMSP', 'RWAT', \\\n",
    "                     'RWATPR', 'SINK', 'STOV', 'TEN', 'KIT', 'MV', 'PLM', 'PLMPRP', \\\n",
    "                     'HOTWAT', 'SRNT', 'SVAL', 'YBL', 'CONP', 'ELEFP', 'ELEP', \\\n",
    "                     'FULFP', 'FULP', 'GASFP', 'GASP', 'HFL', 'INSP', 'MHP', \\\n",
    "                     'RNTM', 'RNTP', 'WATFP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "    data_frame = data_frame[relevant_cols]\n",
    "    data_frame = data_frame.dropna(subset = ['FS', 'HINCP'])    \n",
    "    data_frame = imptr.fit_transform(data_frame)\n",
    "    return pd.DataFrame(data_frame, columns = relevant_cols)\n",
    "    \n",
    "# Import the nationwide data-it is presented in two files \n",
    "file_dir = '/Users/flatironschol/FIS-Projects/Module5/data/'\n",
    "df = pd.read_csv('psam_h06.csv')\n",
    "df = drop_weights(df)\n",
    "df = drop_allocations(df)\n",
    "df = recode(df)\n",
    "df = imputer(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More pre-processing: scaling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test train split.\n",
      "Finished scaling.\n",
      "Encoded PUMA_101.\n",
      "Encoded PUMA_102.\n",
      "Encoded PUMA_103.\n",
      "Encoded PUMA_104.\n",
      "Encoded PUMA_105.\n",
      "Encoded PUMA_106.\n",
      "Encoded PUMA_107.\n",
      "Encoded PUMA_108.\n",
      "Encoded PUMA_109.\n",
      "Encoded PUMA_110.\n",
      "Encoded PUMA_300.\n",
      "Encoded PUMA_701.\n",
      "Encoded PUMA_702.\n",
      "Encoded PUMA_1100.\n",
      "Encoded PUMA_1301.\n",
      "Encoded PUMA_1302.\n",
      "Encoded PUMA_1303.\n",
      "Encoded PUMA_1304.\n",
      "Encoded PUMA_1305.\n",
      "Encoded PUMA_1306.\n",
      "Encoded PUMA_1307.\n",
      "Encoded PUMA_1308.\n",
      "Encoded PUMA_1309.\n",
      "Encoded PUMA_1500.\n",
      "Encoded PUMA_1700.\n",
      "Encoded PUMA_1901.\n",
      "Encoded PUMA_1902.\n",
      "Encoded PUMA_1903.\n",
      "Encoded PUMA_1904.\n",
      "Encoded PUMA_1905.\n",
      "Encoded PUMA_1906.\n",
      "Encoded PUMA_1907.\n",
      "Encoded PUMA_2300.\n",
      "Encoded PUMA_2500.\n",
      "Encoded PUMA_2901.\n",
      "Encoded PUMA_2902.\n",
      "Encoded PUMA_2903.\n",
      "Encoded PUMA_2904.\n",
      "Encoded PUMA_2905.\n",
      "Encoded PUMA_3100.\n",
      "Encoded PUMA_3300.\n",
      "Encoded PUMA_3701.\n",
      "Encoded PUMA_3702.\n",
      "Encoded PUMA_3703.\n",
      "Encoded PUMA_3704.\n",
      "Encoded PUMA_3705.\n",
      "Encoded PUMA_3706.\n",
      "Encoded PUMA_3707.\n",
      "Encoded PUMA_3708.\n",
      "Encoded PUMA_3709.\n",
      "Encoded PUMA_3710.\n",
      "Encoded PUMA_3711.\n",
      "Encoded PUMA_3712.\n",
      "Encoded PUMA_3713.\n",
      "Encoded PUMA_3714.\n",
      "Encoded PUMA_3715.\n",
      "Encoded PUMA_3716.\n",
      "Encoded PUMA_3717.\n",
      "Encoded PUMA_3718.\n",
      "Encoded PUMA_3719.\n",
      "Encoded PUMA_3720.\n",
      "Encoded PUMA_3721.\n",
      "Encoded PUMA_3722.\n",
      "Encoded PUMA_3723.\n",
      "Encoded PUMA_3724.\n",
      "Encoded PUMA_3725.\n",
      "Encoded PUMA_3726.\n",
      "Encoded PUMA_3727.\n",
      "Encoded PUMA_3728.\n",
      "Encoded PUMA_3729.\n",
      "Encoded PUMA_3730.\n",
      "Encoded PUMA_3731.\n",
      "Encoded PUMA_3732.\n",
      "Encoded PUMA_3733.\n",
      "Encoded PUMA_3734.\n",
      "Encoded PUMA_3735.\n",
      "Encoded PUMA_3736.\n",
      "Encoded PUMA_3737.\n",
      "Encoded PUMA_3738.\n",
      "Encoded PUMA_3739.\n",
      "Encoded PUMA_3740.\n",
      "Encoded PUMA_3741.\n",
      "Encoded PUMA_3742.\n",
      "Encoded PUMA_3743.\n",
      "Encoded PUMA_3744.\n",
      "Encoded PUMA_3745.\n",
      "Encoded PUMA_3746.\n",
      "Encoded PUMA_3747.\n",
      "Encoded PUMA_3748.\n",
      "Encoded PUMA_3749.\n",
      "Encoded PUMA_3750.\n",
      "Encoded PUMA_3751.\n",
      "Encoded PUMA_3752.\n",
      "Encoded PUMA_3753.\n",
      "Encoded PUMA_3754.\n",
      "Encoded PUMA_3755.\n",
      "Encoded PUMA_3756.\n",
      "Encoded PUMA_3757.\n",
      "Encoded PUMA_3758.\n",
      "Encoded PUMA_3759.\n",
      "Encoded PUMA_3760.\n",
      "Encoded PUMA_3761.\n",
      "Encoded PUMA_3762.\n",
      "Encoded PUMA_3763.\n",
      "Encoded PUMA_3764.\n",
      "Encoded PUMA_3765.\n",
      "Encoded PUMA_3766.\n",
      "Encoded PUMA_3767.\n",
      "Encoded PUMA_3768.\n",
      "Encoded PUMA_3769.\n",
      "Encoded PUMA_3900.\n",
      "Encoded PUMA_4101.\n",
      "Encoded PUMA_4102.\n",
      "Encoded PUMA_4701.\n",
      "Encoded PUMA_4702.\n",
      "Encoded PUMA_5301.\n",
      "Encoded PUMA_5302.\n",
      "Encoded PUMA_5303.\n",
      "Encoded PUMA_5500.\n",
      "Encoded PUMA_5700.\n",
      "Encoded PUMA_5901.\n",
      "Encoded PUMA_5902.\n",
      "Encoded PUMA_5903.\n",
      "Encoded PUMA_5904.\n",
      "Encoded PUMA_5905.\n",
      "Encoded PUMA_5906.\n",
      "Encoded PUMA_5907.\n",
      "Encoded PUMA_5908.\n",
      "Encoded PUMA_5909.\n",
      "Encoded PUMA_5910.\n",
      "Encoded PUMA_5911.\n",
      "Encoded PUMA_5912.\n",
      "Encoded PUMA_5913.\n",
      "Encoded PUMA_5914.\n",
      "Encoded PUMA_5915.\n",
      "Encoded PUMA_5916.\n",
      "Encoded PUMA_5917.\n",
      "Encoded PUMA_5918.\n",
      "Encoded PUMA_6101.\n",
      "Encoded PUMA_6102.\n",
      "Encoded PUMA_6103.\n",
      "Encoded PUMA_6501.\n",
      "Encoded PUMA_6502.\n",
      "Encoded PUMA_6503.\n",
      "Encoded PUMA_6504.\n",
      "Encoded PUMA_6505.\n",
      "Encoded PUMA_6506.\n",
      "Encoded PUMA_6507.\n",
      "Encoded PUMA_6508.\n",
      "Encoded PUMA_6509.\n",
      "Encoded PUMA_6510.\n",
      "Encoded PUMA_6511.\n",
      "Encoded PUMA_6512.\n",
      "Encoded PUMA_6513.\n",
      "Encoded PUMA_6514.\n",
      "Encoded PUMA_6515.\n",
      "Encoded PUMA_6701.\n",
      "Encoded PUMA_6702.\n",
      "Encoded PUMA_6703.\n",
      "Encoded PUMA_6704.\n",
      "Encoded PUMA_6705.\n",
      "Encoded PUMA_6706.\n",
      "Encoded PUMA_6707.\n",
      "Encoded PUMA_6708.\n",
      "Encoded PUMA_6709.\n",
      "Encoded PUMA_6710.\n",
      "Encoded PUMA_6711.\n",
      "Encoded PUMA_6712.\n",
      "Encoded PUMA_7101.\n",
      "Encoded PUMA_7102.\n",
      "Encoded PUMA_7103.\n",
      "Encoded PUMA_7104.\n",
      "Encoded PUMA_7105.\n",
      "Encoded PUMA_7106.\n",
      "Encoded PUMA_7107.\n",
      "Encoded PUMA_7108.\n",
      "Encoded PUMA_7109.\n",
      "Encoded PUMA_7110.\n",
      "Encoded PUMA_7111.\n",
      "Encoded PUMA_7112.\n",
      "Encoded PUMA_7113.\n",
      "Encoded PUMA_7114.\n",
      "Encoded PUMA_7115.\n",
      "Encoded PUMA_7301.\n",
      "Encoded PUMA_7302.\n",
      "Encoded PUMA_7303.\n",
      "Encoded PUMA_7304.\n",
      "Encoded PUMA_7305.\n",
      "Encoded PUMA_7306.\n",
      "Encoded PUMA_7307.\n",
      "Encoded PUMA_7308.\n",
      "Encoded PUMA_7309.\n",
      "Encoded PUMA_7310.\n",
      "Encoded PUMA_7311.\n",
      "Encoded PUMA_7312.\n",
      "Encoded PUMA_7313.\n",
      "Encoded PUMA_7314.\n",
      "Encoded PUMA_7315.\n",
      "Encoded PUMA_7316.\n",
      "Encoded PUMA_7317.\n",
      "Encoded PUMA_7318.\n",
      "Encoded PUMA_7319.\n",
      "Encoded PUMA_7320.\n",
      "Encoded PUMA_7321.\n",
      "Encoded PUMA_7322.\n",
      "Encoded PUMA_7501.\n",
      "Encoded PUMA_7502.\n",
      "Encoded PUMA_7503.\n",
      "Encoded PUMA_7504.\n",
      "Encoded PUMA_7505.\n",
      "Encoded PUMA_7506.\n",
      "Encoded PUMA_7507.\n",
      "Encoded PUMA_7701.\n",
      "Encoded PUMA_7702.\n",
      "Encoded PUMA_7703.\n",
      "Encoded PUMA_7704.\n",
      "Encoded PUMA_7901.\n",
      "Encoded PUMA_7902.\n",
      "Encoded PUMA_8101.\n",
      "Encoded PUMA_8102.\n",
      "Encoded PUMA_8103.\n",
      "Encoded PUMA_8104.\n",
      "Encoded PUMA_8105.\n",
      "Encoded PUMA_8106.\n",
      "Encoded PUMA_8301.\n",
      "Encoded PUMA_8302.\n",
      "Encoded PUMA_8303.\n",
      "Encoded PUMA_8501.\n",
      "Encoded PUMA_8502.\n",
      "Encoded PUMA_8503.\n",
      "Encoded PUMA_8504.\n",
      "Encoded PUMA_8505.\n",
      "Encoded PUMA_8506.\n",
      "Encoded PUMA_8507.\n",
      "Encoded PUMA_8508.\n",
      "Encoded PUMA_8509.\n",
      "Encoded PUMA_8510.\n",
      "Encoded PUMA_8511.\n",
      "Encoded PUMA_8512.\n",
      "Encoded PUMA_8513.\n",
      "Encoded PUMA_8514.\n",
      "Encoded PUMA_8701.\n",
      "Encoded PUMA_8702.\n",
      "Encoded PUMA_8900.\n",
      "Encoded PUMA_9501.\n",
      "Encoded PUMA_9502.\n",
      "Encoded PUMA_9503.\n",
      "Encoded PUMA_9701.\n",
      "Encoded PUMA_9702.\n",
      "Encoded PUMA_9703.\n",
      "Encoded PUMA_9901.\n",
      "Encoded PUMA_9902.\n",
      "Encoded PUMA_9903.\n",
      "Encoded PUMA_9904.\n",
      "Encoded PUMA_10100.\n",
      "Encoded PUMA_10701.\n",
      "Encoded PUMA_10702.\n",
      "Encoded PUMA_10703.\n",
      "Encoded PUMA_11101.\n",
      "Encoded PUMA_11102.\n",
      "Encoded PUMA_11103.\n",
      "Encoded PUMA_11104.\n",
      "Encoded PUMA_11105.\n",
      "Encoded PUMA_11106.\n",
      "Encoded PUMA_11300.\n",
      "Encoded ADJINC_1013097.\n",
      "Encoded WIF_0.\n",
      "Encoded WIF_1.0.\n",
      "Encoded WIF_2.0.\n",
      "Encoded WIF_3.0.\n",
      "Encoded WORKSTAT_0.\n",
      "Encoded WORKSTAT_1.0.\n",
      "Encoded WORKSTAT_2.0.\n",
      "Encoded WORKSTAT_3.0.\n",
      "Encoded WORKSTAT_4.0.\n",
      "Encoded WORKSTAT_5.0.\n",
      "Encoded WORKSTAT_6.0.\n",
      "Encoded WORKSTAT_7.0.\n",
      "Encoded WORKSTAT_8.0.\n",
      "Encoded WORKSTAT_9.0.\n",
      "Encoded WORKSTAT_10.0.\n",
      "Encoded WORKSTAT_11.0.\n",
      "Encoded WORKSTAT_12.0.\n",
      "Encoded WORKSTAT_13.0.\n",
      "Encoded WORKSTAT_14.0.\n",
      "Encoded WORKSTAT_15.0.\n",
      "Encoded HHL_1.0.\n",
      "Encoded HHL_2.0.\n",
      "Encoded HHL_3.0.\n",
      "Encoded HHL_4.0.\n",
      "Encoded HHL_5.0.\n",
      "Encoded FPARC_0.\n",
      "Encoded FPARC_1.0.\n",
      "Encoded FPARC_2.0.\n",
      "Encoded FPARC_3.0.\n",
      "Encoded FPARC_4.0.\n",
      "Encoded HHT_1.0.\n",
      "Encoded HHT_2.0.\n",
      "Encoded HHT_3.0.\n",
      "Encoded HHT_4.0.\n",
      "Encoded HHT_5.0.\n",
      "Encoded HHT_6.0.\n",
      "Encoded HHT_7.0.\n",
      "Encoded HUGCL_1.0.\n",
      "Encoded HUGCL_2.0.\n",
      "Encoded HUPAC_1.0.\n",
      "Encoded HUPAC_2.0.\n",
      "Encoded HUPAC_3.0.\n",
      "Encoded HUPAC_4.0.\n",
      "Encoded LNGI_1.0.\n",
      "Encoded LNGI_2.0.\n",
      "Encoded MULTG_1.0.\n",
      "Encoded MULTG_2.0.\n",
      "Encoded NPP_1.0.\n",
      "Encoded NPP_2.0.\n",
      "Encoded NR_1.0.\n",
      "Encoded NR_2.0.\n",
      "Encoded PARTNER_0.0.\n",
      "Encoded PARTNER_1.0.\n",
      "Encoded PARTNER_2.0.\n",
      "Encoded PARTNER_3.0.\n",
      "Encoded PARTNER_4.0.\n",
      "Encoded PSF_1.0.\n",
      "Encoded PSF_2.0.\n",
      "Encoded R18_1.0.\n",
      "Encoded R18_2.0.\n",
      "Encoded R65_0.0.\n",
      "Encoded R65_1.0.\n",
      "Encoded R65_2.0.\n",
      "Encoded SSMC_0.0.\n",
      "Encoded SSMC_1.0.\n",
      "Encoded SSMC_2.0.\n",
      "Encoded ACCESS_1.0.\n",
      "Encoded ACCESS_2.0.\n",
      "Encoded ACCESS_3.0.\n",
      "Encoded BROADBND_0.\n",
      "Encoded BROADBND_1.0.\n",
      "Encoded BROADBND_2.0.\n",
      "Encoded COMPOTHX_1.0.\n",
      "Encoded COMPOTHX_2.0.\n",
      "Encoded DIALUP_0.\n",
      "Encoded DIALUP_1.0.\n",
      "Encoded DIALUP_2.0.\n",
      "Encoded HISPEED_0.\n",
      "Encoded HISPEED_1.0.\n",
      "Encoded HISPEED_2.0.\n",
      "Encoded LAPTOP_1.0.\n",
      "Encoded LAPTOP_2.0.\n",
      "Encoded OTHSVCEX_0.\n",
      "Encoded OTHSVCEX_1.0.\n",
      "Encoded OTHSVCEX_2.0.\n",
      "Encoded SATELLITE_0.\n",
      "Encoded SATELLITE_1.0.\n",
      "Encoded SATELLITE_2.0.\n",
      "Encoded SMARTPHONE_1.0.\n",
      "Encoded SMARTPHONE_2.0.\n",
      "Encoded TABLET_1.0.\n",
      "Encoded TABLET_2.0.\n",
      "Encoded TEL_1.0.\n",
      "Encoded TEL_2.0.\n",
      "Encoded TYPE_1.\n",
      "Encoded BATH_1.0.\n",
      "Encoded BATH_2.0.\n",
      "Encoded REFR_1.0.\n",
      "Encoded REFR_2.0.\n",
      "Encoded RWAT_1.0.\n",
      "Encoded RWAT_2.0.\n",
      "Encoded SINK_1.0.\n",
      "Encoded SINK_2.0.\n",
      "Encoded STOV_1.0.\n",
      "Encoded STOV_2.0.\n",
      "Encoded TEN_1.0.\n",
      "Encoded TEN_2.0.\n",
      "Encoded TEN_3.0.\n",
      "Encoded TEN_4.0.\n",
      "Encoded KIT_1.0.\n",
      "Encoded KIT_2.0.\n",
      "Encoded MV_1.0.\n",
      "Encoded MV_2.0.\n",
      "Encoded MV_3.0.\n",
      "Encoded MV_4.0.\n",
      "Encoded MV_5.0.\n",
      "Encoded MV_6.0.\n",
      "Encoded MV_7.0.\n",
      "Encoded PLM_1.0.\n",
      "Encoded PLM_2.0.\n",
      "Encoded SRNT_1.0.\n",
      "Encoded SRNT_2.0.\n",
      "Encoded SVAL_1.0.\n",
      "Encoded SVAL_2.0.\n",
      "Encoded ELEFP_1.0.\n",
      "Encoded ELEFP_2.0.\n",
      "Encoded ELEFP_3.0.\n",
      "Encoded FULFP_1.0.\n",
      "Encoded FULFP_2.0.\n",
      "Encoded FULFP_3.0.\n",
      "Encoded GASFP_0.\n",
      "Encoded GASFP_1.0.\n",
      "Encoded GASFP_2.0.\n",
      "Encoded GASFP_3.0.\n",
      "Encoded HFL_1.0.\n",
      "Encoded HFL_2.0.\n",
      "Encoded HFL_3.0.\n",
      "Encoded HFL_4.0.\n",
      "Encoded HFL_5.0.\n",
      "Encoded HFL_6.0.\n",
      "Encoded HFL_7.0.\n",
      "Encoded HFL_8.0.\n",
      "Encoded HFL_9.0.\n",
      "Encoded RNTM_0.\n",
      "Encoded RNTM_1.0.\n",
      "Encoded RNTM_2.0.\n",
      "Encoded WATFP_1.0.\n",
      "Encoded WATFP_2.0.\n",
      "Encoded WATFP_3.0.\n",
      "Encoded PUMA_101.\n",
      "Encoded PUMA_102.\n",
      "Encoded PUMA_103.\n",
      "Encoded PUMA_104.\n",
      "Encoded PUMA_105.\n",
      "Encoded PUMA_106.\n",
      "Encoded PUMA_107.\n",
      "Encoded PUMA_108.\n",
      "Encoded PUMA_109.\n",
      "Encoded PUMA_110.\n",
      "Encoded PUMA_300.\n",
      "Encoded PUMA_701.\n",
      "Encoded PUMA_702.\n",
      "Encoded PUMA_1100.\n",
      "Encoded PUMA_1301.\n",
      "Encoded PUMA_1302.\n",
      "Encoded PUMA_1303.\n",
      "Encoded PUMA_1304.\n",
      "Encoded PUMA_1305.\n",
      "Encoded PUMA_1306.\n",
      "Encoded PUMA_1307.\n",
      "Encoded PUMA_1308.\n",
      "Encoded PUMA_1309.\n",
      "Encoded PUMA_1500.\n",
      "Encoded PUMA_1700.\n",
      "Encoded PUMA_1901.\n",
      "Encoded PUMA_1902.\n",
      "Encoded PUMA_1903.\n",
      "Encoded PUMA_1904.\n",
      "Encoded PUMA_1905.\n",
      "Encoded PUMA_1906.\n",
      "Encoded PUMA_1907.\n",
      "Encoded PUMA_2300.\n",
      "Encoded PUMA_2500.\n",
      "Encoded PUMA_2901.\n",
      "Encoded PUMA_2902.\n",
      "Encoded PUMA_2903.\n",
      "Encoded PUMA_2904.\n",
      "Encoded PUMA_2905.\n",
      "Encoded PUMA_3100.\n",
      "Encoded PUMA_3300.\n",
      "Encoded PUMA_3701.\n",
      "Encoded PUMA_3702.\n",
      "Encoded PUMA_3703.\n",
      "Encoded PUMA_3704.\n",
      "Encoded PUMA_3705.\n",
      "Encoded PUMA_3706.\n",
      "Encoded PUMA_3707.\n",
      "Encoded PUMA_3708.\n",
      "Encoded PUMA_3709.\n",
      "Encoded PUMA_3710.\n",
      "Encoded PUMA_3711.\n",
      "Encoded PUMA_3712.\n",
      "Encoded PUMA_3713.\n",
      "Encoded PUMA_3714.\n",
      "Encoded PUMA_3715.\n",
      "Encoded PUMA_3716.\n",
      "Encoded PUMA_3717.\n",
      "Encoded PUMA_3718.\n",
      "Encoded PUMA_3719.\n",
      "Encoded PUMA_3720.\n",
      "Encoded PUMA_3721.\n",
      "Encoded PUMA_3722.\n",
      "Encoded PUMA_3723.\n",
      "Encoded PUMA_3724.\n",
      "Encoded PUMA_3725.\n",
      "Encoded PUMA_3726.\n",
      "Encoded PUMA_3727.\n",
      "Encoded PUMA_3728.\n",
      "Encoded PUMA_3729.\n",
      "Encoded PUMA_3730.\n",
      "Encoded PUMA_3731.\n",
      "Encoded PUMA_3732.\n",
      "Encoded PUMA_3733.\n",
      "Encoded PUMA_3734.\n",
      "Encoded PUMA_3735.\n",
      "Encoded PUMA_3736.\n",
      "Encoded PUMA_3737.\n",
      "Encoded PUMA_3738.\n",
      "Encoded PUMA_3739.\n",
      "Encoded PUMA_3740.\n",
      "Encoded PUMA_3741.\n",
      "Encoded PUMA_3742.\n",
      "Encoded PUMA_3743.\n",
      "Encoded PUMA_3744.\n",
      "Encoded PUMA_3745.\n",
      "Encoded PUMA_3746.\n",
      "Encoded PUMA_3747.\n",
      "Encoded PUMA_3748.\n",
      "Encoded PUMA_3749.\n",
      "Encoded PUMA_3750.\n",
      "Encoded PUMA_3751.\n",
      "Encoded PUMA_3752.\n",
      "Encoded PUMA_3753.\n",
      "Encoded PUMA_3754.\n",
      "Encoded PUMA_3755.\n",
      "Encoded PUMA_3756.\n",
      "Encoded PUMA_3757.\n",
      "Encoded PUMA_3758.\n",
      "Encoded PUMA_3759.\n",
      "Encoded PUMA_3760.\n",
      "Encoded PUMA_3761.\n",
      "Encoded PUMA_3762.\n",
      "Encoded PUMA_3763.\n",
      "Encoded PUMA_3764.\n",
      "Encoded PUMA_3765.\n",
      "Encoded PUMA_3766.\n",
      "Encoded PUMA_3767.\n",
      "Encoded PUMA_3768.\n",
      "Encoded PUMA_3769.\n",
      "Encoded PUMA_3900.\n",
      "Encoded PUMA_4101.\n",
      "Encoded PUMA_4102.\n",
      "Encoded PUMA_4701.\n",
      "Encoded PUMA_4702.\n",
      "Encoded PUMA_5301.\n",
      "Encoded PUMA_5302.\n",
      "Encoded PUMA_5303.\n",
      "Encoded PUMA_5500.\n",
      "Encoded PUMA_5700.\n",
      "Encoded PUMA_5901.\n",
      "Encoded PUMA_5902.\n",
      "Encoded PUMA_5903.\n",
      "Encoded PUMA_5904.\n",
      "Encoded PUMA_5905.\n",
      "Encoded PUMA_5906.\n",
      "Encoded PUMA_5907.\n",
      "Encoded PUMA_5908.\n",
      "Encoded PUMA_5909.\n",
      "Encoded PUMA_5910.\n",
      "Encoded PUMA_5911.\n",
      "Encoded PUMA_5912.\n",
      "Encoded PUMA_5913.\n",
      "Encoded PUMA_5914.\n",
      "Encoded PUMA_5915.\n",
      "Encoded PUMA_5916.\n",
      "Encoded PUMA_5917.\n",
      "Encoded PUMA_5918.\n",
      "Encoded PUMA_6101.\n",
      "Encoded PUMA_6102.\n",
      "Encoded PUMA_6103.\n",
      "Encoded PUMA_6501.\n",
      "Encoded PUMA_6502.\n",
      "Encoded PUMA_6503.\n",
      "Encoded PUMA_6504.\n",
      "Encoded PUMA_6505.\n",
      "Encoded PUMA_6506.\n",
      "Encoded PUMA_6507.\n",
      "Encoded PUMA_6508.\n",
      "Encoded PUMA_6509.\n",
      "Encoded PUMA_6510.\n",
      "Encoded PUMA_6511.\n",
      "Encoded PUMA_6512.\n",
      "Encoded PUMA_6513.\n",
      "Encoded PUMA_6514.\n",
      "Encoded PUMA_6515.\n",
      "Encoded PUMA_6701.\n",
      "Encoded PUMA_6702.\n",
      "Encoded PUMA_6703.\n",
      "Encoded PUMA_6704.\n",
      "Encoded PUMA_6705.\n",
      "Encoded PUMA_6706.\n",
      "Encoded PUMA_6707.\n",
      "Encoded PUMA_6708.\n",
      "Encoded PUMA_6709.\n",
      "Encoded PUMA_6710.\n",
      "Encoded PUMA_6711.\n",
      "Encoded PUMA_6712.\n",
      "Encoded PUMA_7101.\n",
      "Encoded PUMA_7102.\n",
      "Encoded PUMA_7103.\n",
      "Encoded PUMA_7104.\n",
      "Encoded PUMA_7105.\n",
      "Encoded PUMA_7106.\n",
      "Encoded PUMA_7107.\n",
      "Encoded PUMA_7108.\n",
      "Encoded PUMA_7109.\n",
      "Encoded PUMA_7110.\n",
      "Encoded PUMA_7111.\n",
      "Encoded PUMA_7112.\n",
      "Encoded PUMA_7113.\n",
      "Encoded PUMA_7114.\n",
      "Encoded PUMA_7115.\n",
      "Encoded PUMA_7301.\n",
      "Encoded PUMA_7302.\n",
      "Encoded PUMA_7303.\n",
      "Encoded PUMA_7304.\n",
      "Encoded PUMA_7305.\n",
      "Encoded PUMA_7306.\n",
      "Encoded PUMA_7307.\n",
      "Encoded PUMA_7308.\n",
      "Encoded PUMA_7309.\n",
      "Encoded PUMA_7310.\n",
      "Encoded PUMA_7311.\n",
      "Encoded PUMA_7312.\n",
      "Encoded PUMA_7313.\n",
      "Encoded PUMA_7314.\n",
      "Encoded PUMA_7315.\n",
      "Encoded PUMA_7316.\n",
      "Encoded PUMA_7317.\n",
      "Encoded PUMA_7318.\n",
      "Encoded PUMA_7319.\n",
      "Encoded PUMA_7320.\n",
      "Encoded PUMA_7321.\n",
      "Encoded PUMA_7322.\n",
      "Encoded PUMA_7501.\n",
      "Encoded PUMA_7502.\n",
      "Encoded PUMA_7503.\n",
      "Encoded PUMA_7504.\n",
      "Encoded PUMA_7505.\n",
      "Encoded PUMA_7506.\n",
      "Encoded PUMA_7507.\n",
      "Encoded PUMA_7701.\n",
      "Encoded PUMA_7702.\n",
      "Encoded PUMA_7703.\n",
      "Encoded PUMA_7704.\n",
      "Encoded PUMA_7901.\n",
      "Encoded PUMA_7902.\n",
      "Encoded PUMA_8101.\n",
      "Encoded PUMA_8102.\n",
      "Encoded PUMA_8103.\n",
      "Encoded PUMA_8104.\n",
      "Encoded PUMA_8105.\n",
      "Encoded PUMA_8106.\n",
      "Encoded PUMA_8301.\n",
      "Encoded PUMA_8302.\n",
      "Encoded PUMA_8303.\n",
      "Encoded PUMA_8501.\n",
      "Encoded PUMA_8502.\n",
      "Encoded PUMA_8503.\n",
      "Encoded PUMA_8504.\n",
      "Encoded PUMA_8505.\n",
      "Encoded PUMA_8506.\n",
      "Encoded PUMA_8507.\n",
      "Encoded PUMA_8508.\n",
      "Encoded PUMA_8509.\n",
      "Encoded PUMA_8510.\n",
      "Encoded PUMA_8511.\n",
      "Encoded PUMA_8512.\n",
      "Encoded PUMA_8513.\n",
      "Encoded PUMA_8514.\n",
      "Encoded PUMA_8701.\n",
      "Encoded PUMA_8702.\n",
      "Encoded PUMA_8900.\n",
      "Encoded PUMA_9501.\n",
      "Encoded PUMA_9502.\n",
      "Encoded PUMA_9503.\n",
      "Encoded PUMA_9701.\n",
      "Encoded PUMA_9702.\n",
      "Encoded PUMA_9703.\n",
      "Encoded PUMA_9901.\n",
      "Encoded PUMA_9902.\n",
      "Encoded PUMA_9903.\n",
      "Encoded PUMA_9904.\n",
      "Encoded PUMA_10100.\n",
      "Encoded PUMA_10701.\n",
      "Encoded PUMA_10702.\n",
      "Encoded PUMA_10703.\n",
      "Encoded PUMA_11101.\n",
      "Encoded PUMA_11102.\n",
      "Encoded PUMA_11103.\n",
      "Encoded PUMA_11104.\n",
      "Encoded PUMA_11105.\n",
      "Encoded PUMA_11106.\n",
      "Encoded PUMA_11300.\n",
      "Encoded ADJINC_1013097.\n",
      "Encoded WIF_0.\n",
      "Encoded WIF_1.0.\n",
      "Encoded WIF_2.0.\n",
      "Encoded WIF_3.0.\n",
      "Encoded WORKSTAT_0.\n",
      "Encoded WORKSTAT_1.0.\n",
      "Encoded WORKSTAT_2.0.\n",
      "Encoded WORKSTAT_3.0.\n",
      "Encoded WORKSTAT_4.0.\n",
      "Encoded WORKSTAT_5.0.\n",
      "Encoded WORKSTAT_6.0.\n",
      "Encoded WORKSTAT_7.0.\n",
      "Encoded WORKSTAT_8.0.\n",
      "Encoded WORKSTAT_9.0.\n",
      "Encoded WORKSTAT_10.0.\n",
      "Encoded WORKSTAT_11.0.\n",
      "Encoded WORKSTAT_12.0.\n",
      "Encoded WORKSTAT_13.0.\n",
      "Encoded WORKSTAT_14.0.\n",
      "Encoded WORKSTAT_15.0.\n",
      "Encoded HHL_1.0.\n",
      "Encoded HHL_2.0.\n",
      "Encoded HHL_3.0.\n",
      "Encoded HHL_4.0.\n",
      "Encoded HHL_5.0.\n",
      "Encoded FPARC_0.\n",
      "Encoded FPARC_1.0.\n",
      "Encoded FPARC_2.0.\n",
      "Encoded FPARC_3.0.\n",
      "Encoded FPARC_4.0.\n",
      "Encoded HHT_1.0.\n",
      "Encoded HHT_2.0.\n",
      "Encoded HHT_3.0.\n",
      "Encoded HHT_4.0.\n",
      "Encoded HHT_5.0.\n",
      "Encoded HHT_6.0.\n",
      "Encoded HHT_7.0.\n",
      "Encoded HUGCL_1.0.\n",
      "Encoded HUGCL_2.0.\n",
      "Encoded HUPAC_1.0.\n",
      "Encoded HUPAC_2.0.\n",
      "Encoded HUPAC_3.0.\n",
      "Encoded HUPAC_4.0.\n",
      "Encoded LNGI_1.0.\n",
      "Encoded LNGI_2.0.\n",
      "Encoded MULTG_1.0.\n",
      "Encoded MULTG_2.0.\n",
      "Encoded NPP_1.0.\n",
      "Encoded NPP_2.0.\n",
      "Encoded NR_1.0.\n",
      "Encoded NR_2.0.\n",
      "Encoded PARTNER_0.0.\n",
      "Encoded PARTNER_1.0.\n",
      "Encoded PARTNER_2.0.\n",
      "Encoded PARTNER_3.0.\n",
      "Encoded PARTNER_4.0.\n",
      "Encoded PSF_1.0.\n",
      "Encoded PSF_2.0.\n",
      "Encoded R18_1.0.\n",
      "Encoded R18_2.0.\n",
      "Encoded R65_0.0.\n",
      "Encoded R65_1.0.\n",
      "Encoded R65_2.0.\n",
      "Encoded SSMC_0.0.\n",
      "Encoded SSMC_1.0.\n",
      "Encoded SSMC_2.0.\n",
      "Encoded ACCESS_1.0.\n",
      "Encoded ACCESS_2.0.\n",
      "Encoded ACCESS_3.0.\n",
      "Encoded BROADBND_0.\n",
      "Encoded BROADBND_1.0.\n",
      "Encoded BROADBND_2.0.\n",
      "Encoded COMPOTHX_1.0.\n",
      "Encoded COMPOTHX_2.0.\n",
      "Encoded DIALUP_0.\n",
      "Encoded DIALUP_1.0.\n",
      "Encoded DIALUP_2.0.\n",
      "Encoded HISPEED_0.\n",
      "Encoded HISPEED_1.0.\n",
      "Encoded HISPEED_2.0.\n",
      "Encoded LAPTOP_1.0.\n",
      "Encoded LAPTOP_2.0.\n",
      "Encoded OTHSVCEX_0.\n",
      "Encoded OTHSVCEX_1.0.\n",
      "Encoded OTHSVCEX_2.0.\n",
      "Encoded SATELLITE_0.\n",
      "Encoded SATELLITE_1.0.\n",
      "Encoded SATELLITE_2.0.\n",
      "Encoded SMARTPHONE_1.0.\n",
      "Encoded SMARTPHONE_2.0.\n",
      "Encoded TABLET_1.0.\n",
      "Encoded TABLET_2.0.\n",
      "Encoded TEL_1.0.\n",
      "Encoded TEL_2.0.\n",
      "Encoded TYPE_1.\n",
      "Encoded BATH_1.0.\n",
      "Encoded BATH_2.0.\n",
      "Encoded REFR_1.0.\n",
      "Encoded REFR_2.0.\n",
      "Encoded RWAT_1.0.\n",
      "Encoded RWAT_2.0.\n",
      "Encoded SINK_1.0.\n",
      "Encoded SINK_2.0.\n",
      "Encoded STOV_1.0.\n",
      "Encoded STOV_2.0.\n",
      "Encoded TEN_1.0.\n",
      "Encoded TEN_2.0.\n",
      "Encoded TEN_3.0.\n",
      "Encoded TEN_4.0.\n",
      "Encoded KIT_1.0.\n",
      "Encoded KIT_2.0.\n",
      "Encoded MV_1.0.\n",
      "Encoded MV_2.0.\n",
      "Encoded MV_3.0.\n",
      "Encoded MV_4.0.\n",
      "Encoded MV_5.0.\n",
      "Encoded MV_6.0.\n",
      "Encoded MV_7.0.\n",
      "Encoded PLM_1.0.\n",
      "Encoded PLM_2.0.\n",
      "Encoded SRNT_1.0.\n",
      "Encoded SRNT_2.0.\n",
      "Encoded SVAL_1.0.\n",
      "Encoded SVAL_2.0.\n",
      "Encoded ELEFP_1.0.\n",
      "Encoded ELEFP_2.0.\n",
      "Encoded ELEFP_3.0.\n",
      "Encoded FULFP_1.0.\n",
      "Encoded FULFP_2.0.\n",
      "Encoded FULFP_3.0.\n",
      "Encoded GASFP_0.\n",
      "Encoded GASFP_1.0.\n",
      "Encoded GASFP_2.0.\n",
      "Encoded GASFP_3.0.\n",
      "Encoded HFL_1.0.\n",
      "Encoded HFL_2.0.\n",
      "Encoded HFL_3.0.\n",
      "Encoded HFL_4.0.\n",
      "Encoded HFL_5.0.\n",
      "Encoded HFL_6.0.\n",
      "Encoded HFL_7.0.\n",
      "Encoded HFL_8.0.\n",
      "Encoded HFL_9.0.\n",
      "Encoded RNTM_0.\n",
      "Encoded RNTM_1.0.\n",
      "Encoded RNTM_2.0.\n",
      "Encoded WATFP_1.0.\n",
      "Encoded WATFP_2.0.\n",
      "Encoded WATFP_3.0.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Function to transform categorical variables\n",
    "    \n",
    "def encoder_transform(encoder, X):\n",
    "    X_encoded = encoder.transform(X).toarray()\n",
    "    encoded_feats = list(encoder.get_feature_names())\n",
    "    feats = X.columns\n",
    "    encoded_feats_updated = []\n",
    "    for feat in encoded_feats:\n",
    "        feat_split = feat.split('_')\n",
    "        i = int(feat_split[0][1:])\n",
    "        dummies = feat_split[1]\n",
    "        feat_updated = f'{feats[i]}_{dummies}'\n",
    "        encoded_feats_updated.append(feat_updated)\n",
    "        print(f'Encoded {feat_updated}.')\n",
    "    return pd.DataFrame(X_encoded, columns = encoded_feats_updated)\n",
    "        \n",
    "y = df.FS\n",
    "X = df.drop(['FS', 'SERIALNO', 'REGION', 'DIVISION', 'ST', 'HOTWAT', 'PLMPRP', 'RWATPR'], axis = 1)\n",
    "# Split the data into test and training samples--stratify by SNAP recipiency\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                    stratify = y, \\\n",
    "                                                    test_size = 0.2, \\\n",
    "                                                    random_state = 1007)\n",
    "print(\"Completed test train split.\")\n",
    "# Scaling and one-hot-encoding of the training set\n",
    "cont_feats = ['HINCP', 'VEH', 'NP', 'NPF', 'NRC', 'BDSP', 'BLD', 'RMSP', \\\n",
    "              'YBL', 'CONP', 'ELEP', 'GASP', 'FULP', 'INSP', 'MHP', \\\n",
    "              'RNTP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "sclr = StandardScaler()\n",
    "X_train_cont = pd.DataFrame(sclr.fit_transform(X_train[cont_feats]), \\\n",
    "                            columns = cont_feats)\n",
    "print(\"Finished scaling.\")\n",
    "cat_feats = X_train.drop(cont_feats, axis = 1).columns\n",
    "encdr = OneHotEncoder(handle_unknown = 'ignore')\n",
    "encdr.fit(X_train[cat_feats])\n",
    "X_train_cat = encoder_transform(encdr, X_train[cat_feats])\n",
    "X_train = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "\n",
    "# Scaling and one-hot-encoding of the test set\n",
    "X_test_cont = pd.DataFrame(sclr.transform(X_test[cont_feats]), \\\n",
    "                           columns = cont_feats)\n",
    "X_test_cat = encoder_transform(encdr, X_test[cat_feats])\n",
    "X_test = pd.concat((X_test_cont, X_test_cat), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HINCP</th>\n",
       "      <th>VEH</th>\n",
       "      <th>NP</th>\n",
       "      <th>NPF</th>\n",
       "      <th>NRC</th>\n",
       "      <th>BDSP</th>\n",
       "      <th>BLD</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>YBL</th>\n",
       "      <th>CONP</th>\n",
       "      <th>...</th>\n",
       "      <th>HFL_6.0</th>\n",
       "      <th>HFL_7.0</th>\n",
       "      <th>HFL_8.0</th>\n",
       "      <th>HFL_9.0</th>\n",
       "      <th>RNTM_0</th>\n",
       "      <th>RNTM_1.0</th>\n",
       "      <th>RNTM_2.0</th>\n",
       "      <th>WATFP_1.0</th>\n",
       "      <th>WATFP_2.0</th>\n",
       "      <th>WATFP_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121290</td>\n",
       "      <td>-1.709006</td>\n",
       "      <td>-1.071423</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-2.290219</td>\n",
       "      <td>1.549852</td>\n",
       "      <td>-1.975683</td>\n",
       "      <td>-0.673670</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620209</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.824870</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>0.267488</td>\n",
       "      <td>-1.089291</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170364</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.824870</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>1.442996</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>2.031653</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>-1.071423</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>2.404761</td>\n",
       "      <td>-0.202898</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.252154</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>1.456967</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>3.447962</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HINCP       VEH        NP       NPF       NRC      BDSP       BLD  \\\n",
       "0 -0.121290 -1.709006 -1.071423 -1.169563 -0.561970 -2.290219  1.549852   \n",
       "1 -0.620209  0.006660  0.824870  0.944122  0.440513 -1.434189  0.267488   \n",
       "2 -0.170364  0.864493  0.824870  0.944122  1.442996  0.277871 -0.587421   \n",
       "3  0.001396 -0.851173 -1.071423 -1.169563 -0.561970 -1.434189  2.404761   \n",
       "4 -0.252154 -0.851173  1.456967  1.472543  3.447962  0.277871 -0.587421   \n",
       "\n",
       "       RMSP       YBL      CONP  ...  HFL_6.0  HFL_7.0  HFL_8.0  HFL_9.0  \\\n",
       "0 -1.975683 -0.673670 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "1 -1.089291 -0.072487 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "2  0.240298  2.031653 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "3 -0.202898 -0.072487 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "4  0.240298  0.228104 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   RNTM_0  RNTM_1.0  RNTM_2.0  WATFP_1.0  WATFP_2.0  WATFP_3.0  \n",
       "0     0.0       1.0       0.0        0.0        1.0        0.0  \n",
       "1     0.0       1.0       0.0        1.0        0.0        0.0  \n",
       "2     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "3     0.0       1.0       0.0        0.0        0.0        1.0  \n",
       "4     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the calculate_vif code, but it continues to hang and I can't see why.  Beneath the following cell, I start running vif individually and dropping the max column it returns by hand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X, thresh=100):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        print('Entered while loop')\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        print('Created c')\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        print('Calculated vif')\n",
    "        maxloc = vif.index(max(vif))\n",
    "        print(f'Max = {maxloc} at {max(vif)}')\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X[cols[variables]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual vif and column dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 17 at 183.38035314331293\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.275597\n",
       "1         0.318254\n",
       "2        -0.662864\n",
       "3         0.867467\n",
       "4        -0.662864\n",
       "            ...   \n",
       "107418    1.664092\n",
       "107419   -0.662864\n",
       "107420   -0.023004\n",
       "107421   -0.662864\n",
       "107422   -0.662864\n",
       "Name: GRNTP, Length: 107423, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['GRNTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 2 at 7.861800044133322\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -1.071423\n",
       "1         0.824870\n",
       "2         0.824870\n",
       "3        -1.071423\n",
       "4         1.456967\n",
       "            ...   \n",
       "107418   -1.071423\n",
       "107419   -0.439326\n",
       "107420    2.089065\n",
       "107421    0.824870\n",
       "107422    0.824870\n",
       "Name: NP, Length: 107423, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 4 at 3.5718768321254966\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After identifying the columns with collinearity issues, I drop them from X_test as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns = ['NP', 'GRNTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HINCP</th>\n",
       "      <th>VEH</th>\n",
       "      <th>NPF</th>\n",
       "      <th>NRC</th>\n",
       "      <th>BDSP</th>\n",
       "      <th>BLD</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>YBL</th>\n",
       "      <th>CONP</th>\n",
       "      <th>ELEP</th>\n",
       "      <th>...</th>\n",
       "      <th>HFL_6.0</th>\n",
       "      <th>HFL_7.0</th>\n",
       "      <th>HFL_8.0</th>\n",
       "      <th>HFL_9.0</th>\n",
       "      <th>RNTM_0</th>\n",
       "      <th>RNTM_1.0</th>\n",
       "      <th>RNTM_2.0</th>\n",
       "      <th>WATFP_1.0</th>\n",
       "      <th>WATFP_2.0</th>\n",
       "      <th>WATFP_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121290</td>\n",
       "      <td>-1.709006</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-2.290219</td>\n",
       "      <td>1.549852</td>\n",
       "      <td>-1.975683</td>\n",
       "      <td>-0.673670</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-1.148211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620209</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>0.267488</td>\n",
       "      <td>-1.089291</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170364</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>1.442996</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>2.031653</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>2.404761</td>\n",
       "      <td>-0.202898</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.503327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.252154</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>3.447962</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.226948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HINCP       VEH       NPF       NRC      BDSP       BLD      RMSP  \\\n",
       "0 -0.121290 -1.709006 -1.169563 -0.561970 -2.290219  1.549852 -1.975683   \n",
       "1 -0.620209  0.006660  0.944122  0.440513 -1.434189  0.267488 -1.089291   \n",
       "2 -0.170364  0.864493  0.944122  1.442996  0.277871 -0.587421  0.240298   \n",
       "3  0.001396 -0.851173 -1.169563 -0.561970 -1.434189  2.404761 -0.202898   \n",
       "4 -0.252154 -0.851173  1.472543  3.447962  0.277871 -0.587421  0.240298   \n",
       "\n",
       "        YBL      CONP      ELEP  ...  HFL_6.0  HFL_7.0  HFL_8.0  HFL_9.0  \\\n",
       "0 -0.673670 -0.213087 -1.148211  ...      0.0      0.0      0.0      0.0   \n",
       "1 -0.072487 -0.213087 -0.687579  ...      0.0      0.0      0.0      0.0   \n",
       "2  2.031653 -0.213087 -0.687579  ...      0.0      0.0      0.0      0.0   \n",
       "3 -0.072487 -0.213087 -0.503327  ...      0.0      0.0      0.0      0.0   \n",
       "4  0.228104 -0.213087 -0.226948  ...      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   RNTM_0  RNTM_1.0  RNTM_2.0  WATFP_1.0  WATFP_2.0  WATFP_3.0  \n",
       "0     0.0       1.0       0.0        0.0        1.0        0.0  \n",
       "1     0.0       1.0       0.0        1.0        0.0        0.0  \n",
       "2     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "3     0.0       1.0       0.0        0.0        0.0        1.0  \n",
       "4     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "\n",
       "[5 rows x 432 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running vanilla LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype('int')\n",
    "clf = LogisticRegression(random_state=1007, solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.99      0.96     24819\n",
      "           2       0.60      0.19      0.29      2037\n",
      "\n",
      "    accuracy                           0.93     26856\n",
      "   macro avg       0.77      0.59      0.62     26856\n",
      "weighted avg       0.91      0.93      0.91     26856\n",
      "\n",
      "[[24564   255]\n",
      " [ 1655   382]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_test = y_test.astype('int')\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not terrific.  Let's try a deep GridSearchCV\n",
    "### Note: this section takes a LOOOOONG time.  Skip running the next 4 cells if you'd prefer not to wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty':('l1', 'l2', 'none'), 'C':[0.001, 0.01, 0.1, 1, 10, 100, 1e9]}\n",
    "clfgs = GridSearchCV(clf, parameters, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=1007, solver='saga',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000000000.0],\n",
       "                         'penalty': ('l1', 'l2', 'none')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.99      0.96     24819\n",
      "           2       0.60      0.19      0.29      2037\n",
      "\n",
      "    accuracy                           0.93     26856\n",
      "   macro avg       0.77      0.59      0.62     26856\n",
      "weighted avg       0.91      0.93      0.91     26856\n",
      "\n",
      "[[24564   255]\n",
      " [ 1655   382]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (This is where you'd come back in if you skipped the above GridSearchCV)\n",
    "### EXACTLY the same.  Not great.  The default parameters were best. \n",
    "### The classes are pretty imbalanced though (around 12:1), so SMOTE may be a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE()\n",
    "y_train = y_train.astype('int')\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 1)/len(y_train) # shows classes are evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another vanilla Logreg - the same parameters (C = 1, penalty = l2) came out in GridSearchCV so we'll stick with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=1007, solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.79      0.87     24819\n",
      "           2       0.23      0.77      0.35      2037\n",
      "\n",
      "    accuracy                           0.79     26856\n",
      "   macro avg       0.60      0.78      0.61     26856\n",
      "weighted avg       0.92      0.79      0.83     26856\n",
      "\n",
      "[[19573  5246]\n",
      " [  475  1562]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_test = y_test.astype('int')\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now precision is low but recall is much better.  Let's try to visualize the precision-recall curve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: f1=0.872 auc=0.386\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict probabilities\n",
    "lr_probs = clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = clf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs, pos_label =2)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write a quick function that returns the 10 features with the highest overall influence (as calculated by absolute value of their betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PUMA_8509',\n",
       " 'PUMA_3729',\n",
       " 'NP',\n",
       " 'PUMA_1302',\n",
       " 'PUMA_8506',\n",
       " 'HINCP',\n",
       " 'PUMA_3741',\n",
       " 'PUMA_3718',\n",
       " 'PUMA_3763',\n",
       " 'PUMA_1907']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = np.abs(clf.coef_[0])\n",
    "X_train_df = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "important_features = []\n",
    "for i in range(10):\n",
    "    index = np.where(betas == max(betas))[0][0]\n",
    "    feature = X_train_df.columns[index]\n",
    "    important_features.append(feature)\n",
    "    betas[index] = -1000\n",
    "\n",
    "important_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets check the betas with high positive correlation as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NP',\n",
       " 'PUMA_1907',\n",
       " 'PUMA_6712',\n",
       " 'PUMA_10701',\n",
       " 'PUMA_9703',\n",
       " 'WORKSTAT_3.0',\n",
       " 'PUMA_2902',\n",
       " 'PUMA_3755',\n",
       " 'PUMA_8503',\n",
       " 'PUMA_7507']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "betas = clf.coef_[0]\n",
    "X_train_df = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "important_features = []\n",
    "for i in range(10):\n",
    "    index = np.where(betas == max(betas))[0][0]\n",
    "    feature = X_train_df.columns[index]\n",
    "    important_features.append(feature)\n",
    "    betas[index] = -1000\n",
    "\n",
    "important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Of the 10 highest betas, only NP, PUMA_6712 and PUMA_10701 are positively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's check the ROC curve for the \"2\" class - recipient enrolled in SNAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "probs = clf.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds, pos_label = 2)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
