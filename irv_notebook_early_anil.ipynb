{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@authors: \n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Function to drop weight columns\n",
    "def drop_weights(data_frame):\n",
    "    coltodrop = []\n",
    "    for i in range(1, 81):\n",
    "        coltodrop.append('WGTP' + str(i))\n",
    "    return data_frame.drop(coltodrop, axis = 1)\n",
    "\n",
    "# Function to drop allocation columns\n",
    "def drop_allocations(data_frame):\n",
    "    col = data_frame.columns\n",
    "    coltodrop = []\n",
    "    for c in col:\n",
    "        if (c[0]== 'F') & (c[-1] == 'P') & \\\n",
    "           (c != 'FULFP') & (c != 'FULP') & (c != 'FINCP'): \n",
    "            coltodrop.append(c)\n",
    "    return data_frame.drop(coltodrop, axis = 1)\n",
    "\n",
    "# Function to recode the categorical features \n",
    "def recode(data_frame):\n",
    "    # Binary categories\n",
    "    dict_bin = {0: 1, 1: 2}\n",
    "    bin_cols = ['HUGCL', 'NPP', 'NR', 'PSF', 'R18']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {1: 2, 2: 1}\n",
    "    bin_cols = ['BROADBND', 'COMPOTHX', 'DIALUP', 'HISPEED', 'LAPTOP', 'OTHSVCEX', \\\n",
    "                'SATELLITE', 'SMARTPHONE', 'TABLET', 'TEL', 'FS', 'BATH', 'REFR', \\\n",
    "                'SINK', 'STOV', 'KIT', 'RNTM']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {0: 2, 1: 1}\n",
    "    bin_cols = ['SRNT', 'SVAL']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    # Three categories\n",
    "    dict_bin = {1: 2, 2: 1, 9:0}\n",
    "    bin_cols = ['RWAT', 'RWATPR', 'PLM', 'PLMPRP', 'HOTWAT']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    dict_bin = {1: 2, 2: 1, 3: 3}\n",
    "    bin_cols = ['ELEFP', 'FULFP', 'GASFP', 'WATFP']\n",
    "    for c in bin_cols:\n",
    "        data_frame[c] = data_frame[c].map(dict_bin)\n",
    "    return data_frame\n",
    "    \n",
    "# Customized imputer function\n",
    "# Since fill_value is a constant, we apply the function to the entire dataset\n",
    "def imputer(data_frame):\n",
    "    imptr = SimpleImputer(strategy = 'constant', fill_value = 0)\n",
    "    relevant_cols = ['SERIALNO', 'REGION', 'DIVISION', 'ST', 'PUMA', 'FS', \\\n",
    "                     'HINCP', 'ADJINC', 'WIF', 'WORKSTAT', 'VEH', 'NP', 'HHL', \\\n",
    "                     'FPARC', 'HHT', 'HUGCL', 'HUPAC', 'LNGI', 'MULTG', 'NPF', \\\n",
    "                     'NPP', 'NR', 'NRC', 'PARTNER', 'PSF', 'R18', 'R65', 'SSMC', \\\n",
    "                     'ACCESS', 'BROADBND', 'COMPOTHX', 'DIALUP', 'HISPEED', 'LAPTOP', \\\n",
    "                     'OTHSVCEX', 'SATELLITE', 'SMARTPHONE', 'TABLET', 'TEL', \\\n",
    "                     'TYPE', 'BATH', 'BDSP', 'BLD', 'REFR', 'RMSP', 'RWAT', \\\n",
    "                     'RWATPR', 'SINK', 'STOV', 'TEN', 'KIT', 'MV', 'PLM', 'PLMPRP', \\\n",
    "                     'HOTWAT', 'SRNT', 'SVAL', 'YBL', 'CONP', 'ELEFP', 'ELEP', \\\n",
    "                     'FULFP', 'FULP', 'GASFP', 'GASP', 'HFL', 'INSP', 'MHP', \\\n",
    "                     'RNTM', 'RNTP', 'WATFP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "    data_frame = data_frame[relevant_cols]\n",
    "    data_frame = data_frame.dropna(subset = ['FS', 'HINCP'])    \n",
    "    data_frame = imptr.fit_transform(data_frame)\n",
    "    return pd.DataFrame(data_frame, columns = relevant_cols)\n",
    "    \n",
    "# Import the nationwide data-it is presented in two files \n",
    "file_dir = '/Users/flatironschol/FIS-Projects/Module5/data/'\n",
    "df = pd.read_csv('psam_h06.csv')\n",
    "df = drop_weights(df)\n",
    "df = drop_allocations(df)\n",
    "df = recode(df)\n",
    "df = imputer(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More pre-processing: scaling and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test train split.\n",
      "Finished scaling.\n",
      "Encoded PUMA_101.\n",
      "Encoded PUMA_102.\n",
      "Encoded PUMA_103.\n",
      "Encoded PUMA_104.\n",
      "Encoded PUMA_105.\n",
      "Encoded PUMA_106.\n",
      "Encoded PUMA_107.\n",
      "Encoded PUMA_108.\n",
      "Encoded PUMA_109.\n",
      "Encoded PUMA_110.\n",
      "Encoded PUMA_300.\n",
      "Encoded PUMA_701.\n",
      "Encoded PUMA_702.\n",
      "Encoded PUMA_1100.\n",
      "Encoded PUMA_1301.\n",
      "Encoded PUMA_1302.\n",
      "Encoded PUMA_1303.\n",
      "Encoded PUMA_1304.\n",
      "Encoded PUMA_1305.\n",
      "Encoded PUMA_1306.\n",
      "Encoded PUMA_1307.\n",
      "Encoded PUMA_1308.\n",
      "Encoded PUMA_1309.\n",
      "Encoded PUMA_1500.\n",
      "Encoded PUMA_1700.\n",
      "Encoded PUMA_1901.\n",
      "Encoded PUMA_1902.\n",
      "Encoded PUMA_1903.\n",
      "Encoded PUMA_1904.\n",
      "Encoded PUMA_1905.\n",
      "Encoded PUMA_1906.\n",
      "Encoded PUMA_1907.\n",
      "Encoded PUMA_2300.\n",
      "Encoded PUMA_2500.\n",
      "Encoded PUMA_2901.\n",
      "Encoded PUMA_2902.\n",
      "Encoded PUMA_2903.\n",
      "Encoded PUMA_2904.\n",
      "Encoded PUMA_2905.\n",
      "Encoded PUMA_3100.\n",
      "Encoded PUMA_3300.\n",
      "Encoded PUMA_3701.\n",
      "Encoded PUMA_3702.\n",
      "Encoded PUMA_3703.\n",
      "Encoded PUMA_3704.\n",
      "Encoded PUMA_3705.\n",
      "Encoded PUMA_3706.\n",
      "Encoded PUMA_3707.\n",
      "Encoded PUMA_3708.\n",
      "Encoded PUMA_3709.\n",
      "Encoded PUMA_3710.\n",
      "Encoded PUMA_3711.\n",
      "Encoded PUMA_3712.\n",
      "Encoded PUMA_3713.\n",
      "Encoded PUMA_3714.\n",
      "Encoded PUMA_3715.\n",
      "Encoded PUMA_3716.\n",
      "Encoded PUMA_3717.\n",
      "Encoded PUMA_3718.\n",
      "Encoded PUMA_3719.\n",
      "Encoded PUMA_3720.\n",
      "Encoded PUMA_3721.\n",
      "Encoded PUMA_3722.\n",
      "Encoded PUMA_3723.\n",
      "Encoded PUMA_3724.\n",
      "Encoded PUMA_3725.\n",
      "Encoded PUMA_3726.\n",
      "Encoded PUMA_3727.\n",
      "Encoded PUMA_3728.\n",
      "Encoded PUMA_3729.\n",
      "Encoded PUMA_3730.\n",
      "Encoded PUMA_3731.\n",
      "Encoded PUMA_3732.\n",
      "Encoded PUMA_3733.\n",
      "Encoded PUMA_3734.\n",
      "Encoded PUMA_3735.\n",
      "Encoded PUMA_3736.\n",
      "Encoded PUMA_3737.\n",
      "Encoded PUMA_3738.\n",
      "Encoded PUMA_3739.\n",
      "Encoded PUMA_3740.\n",
      "Encoded PUMA_3741.\n",
      "Encoded PUMA_3742.\n",
      "Encoded PUMA_3743.\n",
      "Encoded PUMA_3744.\n",
      "Encoded PUMA_3745.\n",
      "Encoded PUMA_3746.\n",
      "Encoded PUMA_3747.\n",
      "Encoded PUMA_3748.\n",
      "Encoded PUMA_3749.\n",
      "Encoded PUMA_3750.\n",
      "Encoded PUMA_3751.\n",
      "Encoded PUMA_3752.\n",
      "Encoded PUMA_3753.\n",
      "Encoded PUMA_3754.\n",
      "Encoded PUMA_3755.\n",
      "Encoded PUMA_3756.\n",
      "Encoded PUMA_3757.\n",
      "Encoded PUMA_3758.\n",
      "Encoded PUMA_3759.\n",
      "Encoded PUMA_3760.\n",
      "Encoded PUMA_3761.\n",
      "Encoded PUMA_3762.\n",
      "Encoded PUMA_3763.\n",
      "Encoded PUMA_3764.\n",
      "Encoded PUMA_3765.\n",
      "Encoded PUMA_3766.\n",
      "Encoded PUMA_3767.\n",
      "Encoded PUMA_3768.\n",
      "Encoded PUMA_3769.\n",
      "Encoded PUMA_3900.\n",
      "Encoded PUMA_4101.\n",
      "Encoded PUMA_4102.\n",
      "Encoded PUMA_4701.\n",
      "Encoded PUMA_4702.\n",
      "Encoded PUMA_5301.\n",
      "Encoded PUMA_5302.\n",
      "Encoded PUMA_5303.\n",
      "Encoded PUMA_5500.\n",
      "Encoded PUMA_5700.\n",
      "Encoded PUMA_5901.\n",
      "Encoded PUMA_5902.\n",
      "Encoded PUMA_5903.\n",
      "Encoded PUMA_5904.\n",
      "Encoded PUMA_5905.\n",
      "Encoded PUMA_5906.\n",
      "Encoded PUMA_5907.\n",
      "Encoded PUMA_5908.\n",
      "Encoded PUMA_5909.\n",
      "Encoded PUMA_5910.\n",
      "Encoded PUMA_5911.\n",
      "Encoded PUMA_5912.\n",
      "Encoded PUMA_5913.\n",
      "Encoded PUMA_5914.\n",
      "Encoded PUMA_5915.\n",
      "Encoded PUMA_5916.\n",
      "Encoded PUMA_5917.\n",
      "Encoded PUMA_5918.\n",
      "Encoded PUMA_6101.\n",
      "Encoded PUMA_6102.\n",
      "Encoded PUMA_6103.\n",
      "Encoded PUMA_6501.\n",
      "Encoded PUMA_6502.\n",
      "Encoded PUMA_6503.\n",
      "Encoded PUMA_6504.\n",
      "Encoded PUMA_6505.\n",
      "Encoded PUMA_6506.\n",
      "Encoded PUMA_6507.\n",
      "Encoded PUMA_6508.\n",
      "Encoded PUMA_6509.\n",
      "Encoded PUMA_6510.\n",
      "Encoded PUMA_6511.\n",
      "Encoded PUMA_6512.\n",
      "Encoded PUMA_6513.\n",
      "Encoded PUMA_6514.\n",
      "Encoded PUMA_6515.\n",
      "Encoded PUMA_6701.\n",
      "Encoded PUMA_6702.\n",
      "Encoded PUMA_6703.\n",
      "Encoded PUMA_6704.\n",
      "Encoded PUMA_6705.\n",
      "Encoded PUMA_6706.\n",
      "Encoded PUMA_6707.\n",
      "Encoded PUMA_6708.\n",
      "Encoded PUMA_6709.\n",
      "Encoded PUMA_6710.\n",
      "Encoded PUMA_6711.\n",
      "Encoded PUMA_6712.\n",
      "Encoded PUMA_7101.\n",
      "Encoded PUMA_7102.\n",
      "Encoded PUMA_7103.\n",
      "Encoded PUMA_7104.\n",
      "Encoded PUMA_7105.\n",
      "Encoded PUMA_7106.\n",
      "Encoded PUMA_7107.\n",
      "Encoded PUMA_7108.\n",
      "Encoded PUMA_7109.\n",
      "Encoded PUMA_7110.\n",
      "Encoded PUMA_7111.\n",
      "Encoded PUMA_7112.\n",
      "Encoded PUMA_7113.\n",
      "Encoded PUMA_7114.\n",
      "Encoded PUMA_7115.\n",
      "Encoded PUMA_7301.\n",
      "Encoded PUMA_7302.\n",
      "Encoded PUMA_7303.\n",
      "Encoded PUMA_7304.\n",
      "Encoded PUMA_7305.\n",
      "Encoded PUMA_7306.\n",
      "Encoded PUMA_7307.\n",
      "Encoded PUMA_7308.\n",
      "Encoded PUMA_7309.\n",
      "Encoded PUMA_7310.\n",
      "Encoded PUMA_7311.\n",
      "Encoded PUMA_7312.\n",
      "Encoded PUMA_7313.\n",
      "Encoded PUMA_7314.\n",
      "Encoded PUMA_7315.\n",
      "Encoded PUMA_7316.\n",
      "Encoded PUMA_7317.\n",
      "Encoded PUMA_7318.\n",
      "Encoded PUMA_7319.\n",
      "Encoded PUMA_7320.\n",
      "Encoded PUMA_7321.\n",
      "Encoded PUMA_7322.\n",
      "Encoded PUMA_7501.\n",
      "Encoded PUMA_7502.\n",
      "Encoded PUMA_7503.\n",
      "Encoded PUMA_7504.\n",
      "Encoded PUMA_7505.\n",
      "Encoded PUMA_7506.\n",
      "Encoded PUMA_7507.\n",
      "Encoded PUMA_7701.\n",
      "Encoded PUMA_7702.\n",
      "Encoded PUMA_7703.\n",
      "Encoded PUMA_7704.\n",
      "Encoded PUMA_7901.\n",
      "Encoded PUMA_7902.\n",
      "Encoded PUMA_8101.\n",
      "Encoded PUMA_8102.\n",
      "Encoded PUMA_8103.\n",
      "Encoded PUMA_8104.\n",
      "Encoded PUMA_8105.\n",
      "Encoded PUMA_8106.\n",
      "Encoded PUMA_8301.\n",
      "Encoded PUMA_8302.\n",
      "Encoded PUMA_8303.\n",
      "Encoded PUMA_8501.\n",
      "Encoded PUMA_8502.\n",
      "Encoded PUMA_8503.\n",
      "Encoded PUMA_8504.\n",
      "Encoded PUMA_8505.\n",
      "Encoded PUMA_8506.\n",
      "Encoded PUMA_8507.\n",
      "Encoded PUMA_8508.\n",
      "Encoded PUMA_8509.\n",
      "Encoded PUMA_8510.\n",
      "Encoded PUMA_8511.\n",
      "Encoded PUMA_8512.\n",
      "Encoded PUMA_8513.\n",
      "Encoded PUMA_8514.\n",
      "Encoded PUMA_8701.\n",
      "Encoded PUMA_8702.\n",
      "Encoded PUMA_8900.\n",
      "Encoded PUMA_9501.\n",
      "Encoded PUMA_9502.\n",
      "Encoded PUMA_9503.\n",
      "Encoded PUMA_9701.\n",
      "Encoded PUMA_9702.\n",
      "Encoded PUMA_9703.\n",
      "Encoded PUMA_9901.\n",
      "Encoded PUMA_9902.\n",
      "Encoded PUMA_9903.\n",
      "Encoded PUMA_9904.\n",
      "Encoded PUMA_10100.\n",
      "Encoded PUMA_10701.\n",
      "Encoded PUMA_10702.\n",
      "Encoded PUMA_10703.\n",
      "Encoded PUMA_11101.\n",
      "Encoded PUMA_11102.\n",
      "Encoded PUMA_11103.\n",
      "Encoded PUMA_11104.\n",
      "Encoded PUMA_11105.\n",
      "Encoded PUMA_11106.\n",
      "Encoded PUMA_11300.\n",
      "Encoded ADJINC_1013097.\n",
      "Encoded WIF_0.\n",
      "Encoded WIF_1.0.\n",
      "Encoded WIF_2.0.\n",
      "Encoded WIF_3.0.\n",
      "Encoded WORKSTAT_0.\n",
      "Encoded WORKSTAT_1.0.\n",
      "Encoded WORKSTAT_2.0.\n",
      "Encoded WORKSTAT_3.0.\n",
      "Encoded WORKSTAT_4.0.\n",
      "Encoded WORKSTAT_5.0.\n",
      "Encoded WORKSTAT_6.0.\n",
      "Encoded WORKSTAT_7.0.\n",
      "Encoded WORKSTAT_8.0.\n",
      "Encoded WORKSTAT_9.0.\n",
      "Encoded WORKSTAT_10.0.\n",
      "Encoded WORKSTAT_11.0.\n",
      "Encoded WORKSTAT_12.0.\n",
      "Encoded WORKSTAT_13.0.\n",
      "Encoded WORKSTAT_14.0.\n",
      "Encoded WORKSTAT_15.0.\n",
      "Encoded HHL_1.0.\n",
      "Encoded HHL_2.0.\n",
      "Encoded HHL_3.0.\n",
      "Encoded HHL_4.0.\n",
      "Encoded HHL_5.0.\n",
      "Encoded FPARC_0.\n",
      "Encoded FPARC_1.0.\n",
      "Encoded FPARC_2.0.\n",
      "Encoded FPARC_3.0.\n",
      "Encoded FPARC_4.0.\n",
      "Encoded HHT_1.0.\n",
      "Encoded HHT_2.0.\n",
      "Encoded HHT_3.0.\n",
      "Encoded HHT_4.0.\n",
      "Encoded HHT_5.0.\n",
      "Encoded HHT_6.0.\n",
      "Encoded HHT_7.0.\n",
      "Encoded HUGCL_1.0.\n",
      "Encoded HUGCL_2.0.\n",
      "Encoded HUPAC_1.0.\n",
      "Encoded HUPAC_2.0.\n",
      "Encoded HUPAC_3.0.\n",
      "Encoded HUPAC_4.0.\n",
      "Encoded LNGI_1.0.\n",
      "Encoded LNGI_2.0.\n",
      "Encoded MULTG_1.0.\n",
      "Encoded MULTG_2.0.\n",
      "Encoded NPP_1.0.\n",
      "Encoded NPP_2.0.\n",
      "Encoded NR_1.0.\n",
      "Encoded NR_2.0.\n",
      "Encoded PARTNER_0.0.\n",
      "Encoded PARTNER_1.0.\n",
      "Encoded PARTNER_2.0.\n",
      "Encoded PARTNER_3.0.\n",
      "Encoded PARTNER_4.0.\n",
      "Encoded PSF_1.0.\n",
      "Encoded PSF_2.0.\n",
      "Encoded R18_1.0.\n",
      "Encoded R18_2.0.\n",
      "Encoded R65_0.0.\n",
      "Encoded R65_1.0.\n",
      "Encoded R65_2.0.\n",
      "Encoded SSMC_0.0.\n",
      "Encoded SSMC_1.0.\n",
      "Encoded SSMC_2.0.\n",
      "Encoded ACCESS_1.0.\n",
      "Encoded ACCESS_2.0.\n",
      "Encoded ACCESS_3.0.\n",
      "Encoded BROADBND_0.\n",
      "Encoded BROADBND_1.0.\n",
      "Encoded BROADBND_2.0.\n",
      "Encoded COMPOTHX_1.0.\n",
      "Encoded COMPOTHX_2.0.\n",
      "Encoded DIALUP_0.\n",
      "Encoded DIALUP_1.0.\n",
      "Encoded DIALUP_2.0.\n",
      "Encoded HISPEED_0.\n",
      "Encoded HISPEED_1.0.\n",
      "Encoded HISPEED_2.0.\n",
      "Encoded LAPTOP_1.0.\n",
      "Encoded LAPTOP_2.0.\n",
      "Encoded OTHSVCEX_0.\n",
      "Encoded OTHSVCEX_1.0.\n",
      "Encoded OTHSVCEX_2.0.\n",
      "Encoded SATELLITE_0.\n",
      "Encoded SATELLITE_1.0.\n",
      "Encoded SATELLITE_2.0.\n",
      "Encoded SMARTPHONE_1.0.\n",
      "Encoded SMARTPHONE_2.0.\n",
      "Encoded TABLET_1.0.\n",
      "Encoded TABLET_2.0.\n",
      "Encoded TEL_1.0.\n",
      "Encoded TEL_2.0.\n",
      "Encoded TYPE_1.\n",
      "Encoded BATH_1.0.\n",
      "Encoded BATH_2.0.\n",
      "Encoded REFR_1.0.\n",
      "Encoded REFR_2.0.\n",
      "Encoded RWAT_1.0.\n",
      "Encoded RWAT_2.0.\n",
      "Encoded SINK_1.0.\n",
      "Encoded SINK_2.0.\n",
      "Encoded STOV_1.0.\n",
      "Encoded STOV_2.0.\n",
      "Encoded TEN_1.0.\n",
      "Encoded TEN_2.0.\n",
      "Encoded TEN_3.0.\n",
      "Encoded TEN_4.0.\n",
      "Encoded KIT_1.0.\n",
      "Encoded KIT_2.0.\n",
      "Encoded MV_1.0.\n",
      "Encoded MV_2.0.\n",
      "Encoded MV_3.0.\n",
      "Encoded MV_4.0.\n",
      "Encoded MV_5.0.\n",
      "Encoded MV_6.0.\n",
      "Encoded MV_7.0.\n",
      "Encoded PLM_1.0.\n",
      "Encoded PLM_2.0.\n",
      "Encoded SRNT_1.0.\n",
      "Encoded SRNT_2.0.\n",
      "Encoded SVAL_1.0.\n",
      "Encoded SVAL_2.0.\n",
      "Encoded ELEFP_1.0.\n",
      "Encoded ELEFP_2.0.\n",
      "Encoded ELEFP_3.0.\n",
      "Encoded FULFP_1.0.\n",
      "Encoded FULFP_2.0.\n",
      "Encoded FULFP_3.0.\n",
      "Encoded GASFP_0.\n",
      "Encoded GASFP_1.0.\n",
      "Encoded GASFP_2.0.\n",
      "Encoded GASFP_3.0.\n",
      "Encoded HFL_1.0.\n",
      "Encoded HFL_2.0.\n",
      "Encoded HFL_3.0.\n",
      "Encoded HFL_4.0.\n",
      "Encoded HFL_5.0.\n",
      "Encoded HFL_6.0.\n",
      "Encoded HFL_7.0.\n",
      "Encoded HFL_8.0.\n",
      "Encoded HFL_9.0.\n",
      "Encoded RNTM_0.\n",
      "Encoded RNTM_1.0.\n",
      "Encoded RNTM_2.0.\n",
      "Encoded WATFP_1.0.\n",
      "Encoded WATFP_2.0.\n",
      "Encoded WATFP_3.0.\n",
      "Encoded PUMA_101.\n",
      "Encoded PUMA_102.\n",
      "Encoded PUMA_103.\n",
      "Encoded PUMA_104.\n",
      "Encoded PUMA_105.\n",
      "Encoded PUMA_106.\n",
      "Encoded PUMA_107.\n",
      "Encoded PUMA_108.\n",
      "Encoded PUMA_109.\n",
      "Encoded PUMA_110.\n",
      "Encoded PUMA_300.\n",
      "Encoded PUMA_701.\n",
      "Encoded PUMA_702.\n",
      "Encoded PUMA_1100.\n",
      "Encoded PUMA_1301.\n",
      "Encoded PUMA_1302.\n",
      "Encoded PUMA_1303.\n",
      "Encoded PUMA_1304.\n",
      "Encoded PUMA_1305.\n",
      "Encoded PUMA_1306.\n",
      "Encoded PUMA_1307.\n",
      "Encoded PUMA_1308.\n",
      "Encoded PUMA_1309.\n",
      "Encoded PUMA_1500.\n",
      "Encoded PUMA_1700.\n",
      "Encoded PUMA_1901.\n",
      "Encoded PUMA_1902.\n",
      "Encoded PUMA_1903.\n",
      "Encoded PUMA_1904.\n",
      "Encoded PUMA_1905.\n",
      "Encoded PUMA_1906.\n",
      "Encoded PUMA_1907.\n",
      "Encoded PUMA_2300.\n",
      "Encoded PUMA_2500.\n",
      "Encoded PUMA_2901.\n",
      "Encoded PUMA_2902.\n",
      "Encoded PUMA_2903.\n",
      "Encoded PUMA_2904.\n",
      "Encoded PUMA_2905.\n",
      "Encoded PUMA_3100.\n",
      "Encoded PUMA_3300.\n",
      "Encoded PUMA_3701.\n",
      "Encoded PUMA_3702.\n",
      "Encoded PUMA_3703.\n",
      "Encoded PUMA_3704.\n",
      "Encoded PUMA_3705.\n",
      "Encoded PUMA_3706.\n",
      "Encoded PUMA_3707.\n",
      "Encoded PUMA_3708.\n",
      "Encoded PUMA_3709.\n",
      "Encoded PUMA_3710.\n",
      "Encoded PUMA_3711.\n",
      "Encoded PUMA_3712.\n",
      "Encoded PUMA_3713.\n",
      "Encoded PUMA_3714.\n",
      "Encoded PUMA_3715.\n",
      "Encoded PUMA_3716.\n",
      "Encoded PUMA_3717.\n",
      "Encoded PUMA_3718.\n",
      "Encoded PUMA_3719.\n",
      "Encoded PUMA_3720.\n",
      "Encoded PUMA_3721.\n",
      "Encoded PUMA_3722.\n",
      "Encoded PUMA_3723.\n",
      "Encoded PUMA_3724.\n",
      "Encoded PUMA_3725.\n",
      "Encoded PUMA_3726.\n",
      "Encoded PUMA_3727.\n",
      "Encoded PUMA_3728.\n",
      "Encoded PUMA_3729.\n",
      "Encoded PUMA_3730.\n",
      "Encoded PUMA_3731.\n",
      "Encoded PUMA_3732.\n",
      "Encoded PUMA_3733.\n",
      "Encoded PUMA_3734.\n",
      "Encoded PUMA_3735.\n",
      "Encoded PUMA_3736.\n",
      "Encoded PUMA_3737.\n",
      "Encoded PUMA_3738.\n",
      "Encoded PUMA_3739.\n",
      "Encoded PUMA_3740.\n",
      "Encoded PUMA_3741.\n",
      "Encoded PUMA_3742.\n",
      "Encoded PUMA_3743.\n",
      "Encoded PUMA_3744.\n",
      "Encoded PUMA_3745.\n",
      "Encoded PUMA_3746.\n",
      "Encoded PUMA_3747.\n",
      "Encoded PUMA_3748.\n",
      "Encoded PUMA_3749.\n",
      "Encoded PUMA_3750.\n",
      "Encoded PUMA_3751.\n",
      "Encoded PUMA_3752.\n",
      "Encoded PUMA_3753.\n",
      "Encoded PUMA_3754.\n",
      "Encoded PUMA_3755.\n",
      "Encoded PUMA_3756.\n",
      "Encoded PUMA_3757.\n",
      "Encoded PUMA_3758.\n",
      "Encoded PUMA_3759.\n",
      "Encoded PUMA_3760.\n",
      "Encoded PUMA_3761.\n",
      "Encoded PUMA_3762.\n",
      "Encoded PUMA_3763.\n",
      "Encoded PUMA_3764.\n",
      "Encoded PUMA_3765.\n",
      "Encoded PUMA_3766.\n",
      "Encoded PUMA_3767.\n",
      "Encoded PUMA_3768.\n",
      "Encoded PUMA_3769.\n",
      "Encoded PUMA_3900.\n",
      "Encoded PUMA_4101.\n",
      "Encoded PUMA_4102.\n",
      "Encoded PUMA_4701.\n",
      "Encoded PUMA_4702.\n",
      "Encoded PUMA_5301.\n",
      "Encoded PUMA_5302.\n",
      "Encoded PUMA_5303.\n",
      "Encoded PUMA_5500.\n",
      "Encoded PUMA_5700.\n",
      "Encoded PUMA_5901.\n",
      "Encoded PUMA_5902.\n",
      "Encoded PUMA_5903.\n",
      "Encoded PUMA_5904.\n",
      "Encoded PUMA_5905.\n",
      "Encoded PUMA_5906.\n",
      "Encoded PUMA_5907.\n",
      "Encoded PUMA_5908.\n",
      "Encoded PUMA_5909.\n",
      "Encoded PUMA_5910.\n",
      "Encoded PUMA_5911.\n",
      "Encoded PUMA_5912.\n",
      "Encoded PUMA_5913.\n",
      "Encoded PUMA_5914.\n",
      "Encoded PUMA_5915.\n",
      "Encoded PUMA_5916.\n",
      "Encoded PUMA_5917.\n",
      "Encoded PUMA_5918.\n",
      "Encoded PUMA_6101.\n",
      "Encoded PUMA_6102.\n",
      "Encoded PUMA_6103.\n",
      "Encoded PUMA_6501.\n",
      "Encoded PUMA_6502.\n",
      "Encoded PUMA_6503.\n",
      "Encoded PUMA_6504.\n",
      "Encoded PUMA_6505.\n",
      "Encoded PUMA_6506.\n",
      "Encoded PUMA_6507.\n",
      "Encoded PUMA_6508.\n",
      "Encoded PUMA_6509.\n",
      "Encoded PUMA_6510.\n",
      "Encoded PUMA_6511.\n",
      "Encoded PUMA_6512.\n",
      "Encoded PUMA_6513.\n",
      "Encoded PUMA_6514.\n",
      "Encoded PUMA_6515.\n",
      "Encoded PUMA_6701.\n",
      "Encoded PUMA_6702.\n",
      "Encoded PUMA_6703.\n",
      "Encoded PUMA_6704.\n",
      "Encoded PUMA_6705.\n",
      "Encoded PUMA_6706.\n",
      "Encoded PUMA_6707.\n",
      "Encoded PUMA_6708.\n",
      "Encoded PUMA_6709.\n",
      "Encoded PUMA_6710.\n",
      "Encoded PUMA_6711.\n",
      "Encoded PUMA_6712.\n",
      "Encoded PUMA_7101.\n",
      "Encoded PUMA_7102.\n",
      "Encoded PUMA_7103.\n",
      "Encoded PUMA_7104.\n",
      "Encoded PUMA_7105.\n",
      "Encoded PUMA_7106.\n",
      "Encoded PUMA_7107.\n",
      "Encoded PUMA_7108.\n",
      "Encoded PUMA_7109.\n",
      "Encoded PUMA_7110.\n",
      "Encoded PUMA_7111.\n",
      "Encoded PUMA_7112.\n",
      "Encoded PUMA_7113.\n",
      "Encoded PUMA_7114.\n",
      "Encoded PUMA_7115.\n",
      "Encoded PUMA_7301.\n",
      "Encoded PUMA_7302.\n",
      "Encoded PUMA_7303.\n",
      "Encoded PUMA_7304.\n",
      "Encoded PUMA_7305.\n",
      "Encoded PUMA_7306.\n",
      "Encoded PUMA_7307.\n",
      "Encoded PUMA_7308.\n",
      "Encoded PUMA_7309.\n",
      "Encoded PUMA_7310.\n",
      "Encoded PUMA_7311.\n",
      "Encoded PUMA_7312.\n",
      "Encoded PUMA_7313.\n",
      "Encoded PUMA_7314.\n",
      "Encoded PUMA_7315.\n",
      "Encoded PUMA_7316.\n",
      "Encoded PUMA_7317.\n",
      "Encoded PUMA_7318.\n",
      "Encoded PUMA_7319.\n",
      "Encoded PUMA_7320.\n",
      "Encoded PUMA_7321.\n",
      "Encoded PUMA_7322.\n",
      "Encoded PUMA_7501.\n",
      "Encoded PUMA_7502.\n",
      "Encoded PUMA_7503.\n",
      "Encoded PUMA_7504.\n",
      "Encoded PUMA_7505.\n",
      "Encoded PUMA_7506.\n",
      "Encoded PUMA_7507.\n",
      "Encoded PUMA_7701.\n",
      "Encoded PUMA_7702.\n",
      "Encoded PUMA_7703.\n",
      "Encoded PUMA_7704.\n",
      "Encoded PUMA_7901.\n",
      "Encoded PUMA_7902.\n",
      "Encoded PUMA_8101.\n",
      "Encoded PUMA_8102.\n",
      "Encoded PUMA_8103.\n",
      "Encoded PUMA_8104.\n",
      "Encoded PUMA_8105.\n",
      "Encoded PUMA_8106.\n",
      "Encoded PUMA_8301.\n",
      "Encoded PUMA_8302.\n",
      "Encoded PUMA_8303.\n",
      "Encoded PUMA_8501.\n",
      "Encoded PUMA_8502.\n",
      "Encoded PUMA_8503.\n",
      "Encoded PUMA_8504.\n",
      "Encoded PUMA_8505.\n",
      "Encoded PUMA_8506.\n",
      "Encoded PUMA_8507.\n",
      "Encoded PUMA_8508.\n",
      "Encoded PUMA_8509.\n",
      "Encoded PUMA_8510.\n",
      "Encoded PUMA_8511.\n",
      "Encoded PUMA_8512.\n",
      "Encoded PUMA_8513.\n",
      "Encoded PUMA_8514.\n",
      "Encoded PUMA_8701.\n",
      "Encoded PUMA_8702.\n",
      "Encoded PUMA_8900.\n",
      "Encoded PUMA_9501.\n",
      "Encoded PUMA_9502.\n",
      "Encoded PUMA_9503.\n",
      "Encoded PUMA_9701.\n",
      "Encoded PUMA_9702.\n",
      "Encoded PUMA_9703.\n",
      "Encoded PUMA_9901.\n",
      "Encoded PUMA_9902.\n",
      "Encoded PUMA_9903.\n",
      "Encoded PUMA_9904.\n",
      "Encoded PUMA_10100.\n",
      "Encoded PUMA_10701.\n",
      "Encoded PUMA_10702.\n",
      "Encoded PUMA_10703.\n",
      "Encoded PUMA_11101.\n",
      "Encoded PUMA_11102.\n",
      "Encoded PUMA_11103.\n",
      "Encoded PUMA_11104.\n",
      "Encoded PUMA_11105.\n",
      "Encoded PUMA_11106.\n",
      "Encoded PUMA_11300.\n",
      "Encoded ADJINC_1013097.\n",
      "Encoded WIF_0.\n",
      "Encoded WIF_1.0.\n",
      "Encoded WIF_2.0.\n",
      "Encoded WIF_3.0.\n",
      "Encoded WORKSTAT_0.\n",
      "Encoded WORKSTAT_1.0.\n",
      "Encoded WORKSTAT_2.0.\n",
      "Encoded WORKSTAT_3.0.\n",
      "Encoded WORKSTAT_4.0.\n",
      "Encoded WORKSTAT_5.0.\n",
      "Encoded WORKSTAT_6.0.\n",
      "Encoded WORKSTAT_7.0.\n",
      "Encoded WORKSTAT_8.0.\n",
      "Encoded WORKSTAT_9.0.\n",
      "Encoded WORKSTAT_10.0.\n",
      "Encoded WORKSTAT_11.0.\n",
      "Encoded WORKSTAT_12.0.\n",
      "Encoded WORKSTAT_13.0.\n",
      "Encoded WORKSTAT_14.0.\n",
      "Encoded WORKSTAT_15.0.\n",
      "Encoded HHL_1.0.\n",
      "Encoded HHL_2.0.\n",
      "Encoded HHL_3.0.\n",
      "Encoded HHL_4.0.\n",
      "Encoded HHL_5.0.\n",
      "Encoded FPARC_0.\n",
      "Encoded FPARC_1.0.\n",
      "Encoded FPARC_2.0.\n",
      "Encoded FPARC_3.0.\n",
      "Encoded FPARC_4.0.\n",
      "Encoded HHT_1.0.\n",
      "Encoded HHT_2.0.\n",
      "Encoded HHT_3.0.\n",
      "Encoded HHT_4.0.\n",
      "Encoded HHT_5.0.\n",
      "Encoded HHT_6.0.\n",
      "Encoded HHT_7.0.\n",
      "Encoded HUGCL_1.0.\n",
      "Encoded HUGCL_2.0.\n",
      "Encoded HUPAC_1.0.\n",
      "Encoded HUPAC_2.0.\n",
      "Encoded HUPAC_3.0.\n",
      "Encoded HUPAC_4.0.\n",
      "Encoded LNGI_1.0.\n",
      "Encoded LNGI_2.0.\n",
      "Encoded MULTG_1.0.\n",
      "Encoded MULTG_2.0.\n",
      "Encoded NPP_1.0.\n",
      "Encoded NPP_2.0.\n",
      "Encoded NR_1.0.\n",
      "Encoded NR_2.0.\n",
      "Encoded PARTNER_0.0.\n",
      "Encoded PARTNER_1.0.\n",
      "Encoded PARTNER_2.0.\n",
      "Encoded PARTNER_3.0.\n",
      "Encoded PARTNER_4.0.\n",
      "Encoded PSF_1.0.\n",
      "Encoded PSF_2.0.\n",
      "Encoded R18_1.0.\n",
      "Encoded R18_2.0.\n",
      "Encoded R65_0.0.\n",
      "Encoded R65_1.0.\n",
      "Encoded R65_2.0.\n",
      "Encoded SSMC_0.0.\n",
      "Encoded SSMC_1.0.\n",
      "Encoded SSMC_2.0.\n",
      "Encoded ACCESS_1.0.\n",
      "Encoded ACCESS_2.0.\n",
      "Encoded ACCESS_3.0.\n",
      "Encoded BROADBND_0.\n",
      "Encoded BROADBND_1.0.\n",
      "Encoded BROADBND_2.0.\n",
      "Encoded COMPOTHX_1.0.\n",
      "Encoded COMPOTHX_2.0.\n",
      "Encoded DIALUP_0.\n",
      "Encoded DIALUP_1.0.\n",
      "Encoded DIALUP_2.0.\n",
      "Encoded HISPEED_0.\n",
      "Encoded HISPEED_1.0.\n",
      "Encoded HISPEED_2.0.\n",
      "Encoded LAPTOP_1.0.\n",
      "Encoded LAPTOP_2.0.\n",
      "Encoded OTHSVCEX_0.\n",
      "Encoded OTHSVCEX_1.0.\n",
      "Encoded OTHSVCEX_2.0.\n",
      "Encoded SATELLITE_0.\n",
      "Encoded SATELLITE_1.0.\n",
      "Encoded SATELLITE_2.0.\n",
      "Encoded SMARTPHONE_1.0.\n",
      "Encoded SMARTPHONE_2.0.\n",
      "Encoded TABLET_1.0.\n",
      "Encoded TABLET_2.0.\n",
      "Encoded TEL_1.0.\n",
      "Encoded TEL_2.0.\n",
      "Encoded TYPE_1.\n",
      "Encoded BATH_1.0.\n",
      "Encoded BATH_2.0.\n",
      "Encoded REFR_1.0.\n",
      "Encoded REFR_2.0.\n",
      "Encoded RWAT_1.0.\n",
      "Encoded RWAT_2.0.\n",
      "Encoded SINK_1.0.\n",
      "Encoded SINK_2.0.\n",
      "Encoded STOV_1.0.\n",
      "Encoded STOV_2.0.\n",
      "Encoded TEN_1.0.\n",
      "Encoded TEN_2.0.\n",
      "Encoded TEN_3.0.\n",
      "Encoded TEN_4.0.\n",
      "Encoded KIT_1.0.\n",
      "Encoded KIT_2.0.\n",
      "Encoded MV_1.0.\n",
      "Encoded MV_2.0.\n",
      "Encoded MV_3.0.\n",
      "Encoded MV_4.0.\n",
      "Encoded MV_5.0.\n",
      "Encoded MV_6.0.\n",
      "Encoded MV_7.0.\n",
      "Encoded PLM_1.0.\n",
      "Encoded PLM_2.0.\n",
      "Encoded SRNT_1.0.\n",
      "Encoded SRNT_2.0.\n",
      "Encoded SVAL_1.0.\n",
      "Encoded SVAL_2.0.\n",
      "Encoded ELEFP_1.0.\n",
      "Encoded ELEFP_2.0.\n",
      "Encoded ELEFP_3.0.\n",
      "Encoded FULFP_1.0.\n",
      "Encoded FULFP_2.0.\n",
      "Encoded FULFP_3.0.\n",
      "Encoded GASFP_0.\n",
      "Encoded GASFP_1.0.\n",
      "Encoded GASFP_2.0.\n",
      "Encoded GASFP_3.0.\n",
      "Encoded HFL_1.0.\n",
      "Encoded HFL_2.0.\n",
      "Encoded HFL_3.0.\n",
      "Encoded HFL_4.0.\n",
      "Encoded HFL_5.0.\n",
      "Encoded HFL_6.0.\n",
      "Encoded HFL_7.0.\n",
      "Encoded HFL_8.0.\n",
      "Encoded HFL_9.0.\n",
      "Encoded RNTM_0.\n",
      "Encoded RNTM_1.0.\n",
      "Encoded RNTM_2.0.\n",
      "Encoded WATFP_1.0.\n",
      "Encoded WATFP_2.0.\n",
      "Encoded WATFP_3.0.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Function to transform categorical variables\n",
    "    \n",
    "def encoder_transform(encoder, X):\n",
    "    X_encoded = encoder.transform(X).toarray()\n",
    "    encoded_feats = list(encoder.get_feature_names())\n",
    "    feats = X.columns\n",
    "    encoded_feats_updated = []\n",
    "    for feat in encoded_feats:\n",
    "        feat_split = feat.split('_')\n",
    "        i = int(feat_split[0][1:])\n",
    "        dummies = feat_split[1]\n",
    "        feat_updated = f'{feats[i]}_{dummies}'\n",
    "        encoded_feats_updated.append(feat_updated)\n",
    "        print(f'Encoded {feat_updated}.')\n",
    "    return pd.DataFrame(X_encoded, columns = encoded_feats_updated)\n",
    "        \n",
    "y = df.FS\n",
    "X = df.drop(['FS', 'SERIALNO', 'REGION', 'DIVISION', 'ST', 'HOTWAT', 'PLMPRP', 'RWATPR'], axis = 1)\n",
    "# Split the data into test and training samples--stratify by SNAP recipiency\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                    stratify = y, \\\n",
    "                                                    test_size = 0.2, \\\n",
    "                                                    random_state = 1007)\n",
    "print(\"Completed test train split.\")\n",
    "# Scaling and one-hot-encoding of the training set\n",
    "cont_feats = ['HINCP', 'VEH', 'NP', 'NPF', 'NRC', 'BDSP', 'BLD', 'RMSP', \\\n",
    "              'YBL', 'CONP', 'ELEP', 'GASP', 'FULP', 'INSP', 'MHP', \\\n",
    "              'RNTP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "sclr = StandardScaler()\n",
    "X_train_cont = pd.DataFrame(sclr.fit_transform(X_train[cont_feats]), \\\n",
    "                            columns = cont_feats)\n",
    "print(\"Finished scaling.\")\n",
    "cat_feats = X_train.drop(cont_feats, axis = 1).columns\n",
    "encdr = OneHotEncoder(handle_unknown = 'ignore')\n",
    "encdr.fit(X_train[cat_feats])\n",
    "X_train_cat = encoder_transform(encdr, X_train[cat_feats])\n",
    "X_train = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "\n",
    "# Scaling and one-hot-encoding of the test set\n",
    "X_test_cont = pd.DataFrame(sclr.transform(X_test[cont_feats]), \\\n",
    "                           columns = cont_feats)\n",
    "X_test_cat = encoder_transform(encdr, X_test[cat_feats])\n",
    "X_test = pd.concat((X_test_cont, X_test_cat), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HINCP</th>\n",
       "      <th>VEH</th>\n",
       "      <th>NP</th>\n",
       "      <th>NPF</th>\n",
       "      <th>NRC</th>\n",
       "      <th>BDSP</th>\n",
       "      <th>BLD</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>YBL</th>\n",
       "      <th>CONP</th>\n",
       "      <th>...</th>\n",
       "      <th>HFL_6.0</th>\n",
       "      <th>HFL_7.0</th>\n",
       "      <th>HFL_8.0</th>\n",
       "      <th>HFL_9.0</th>\n",
       "      <th>RNTM_0</th>\n",
       "      <th>RNTM_1.0</th>\n",
       "      <th>RNTM_2.0</th>\n",
       "      <th>WATFP_1.0</th>\n",
       "      <th>WATFP_2.0</th>\n",
       "      <th>WATFP_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121290</td>\n",
       "      <td>-1.709006</td>\n",
       "      <td>-1.071423</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-2.290219</td>\n",
       "      <td>1.549852</td>\n",
       "      <td>-1.975683</td>\n",
       "      <td>-0.673670</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620209</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.824870</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>0.267488</td>\n",
       "      <td>-1.089291</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170364</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.824870</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>1.442996</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>2.031653</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>-1.071423</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>2.404761</td>\n",
       "      <td>-0.202898</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.252154</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>1.456967</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>3.447962</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HINCP       VEH        NP       NPF       NRC      BDSP       BLD  \\\n",
       "0 -0.121290 -1.709006 -1.071423 -1.169563 -0.561970 -2.290219  1.549852   \n",
       "1 -0.620209  0.006660  0.824870  0.944122  0.440513 -1.434189  0.267488   \n",
       "2 -0.170364  0.864493  0.824870  0.944122  1.442996  0.277871 -0.587421   \n",
       "3  0.001396 -0.851173 -1.071423 -1.169563 -0.561970 -1.434189  2.404761   \n",
       "4 -0.252154 -0.851173  1.456967  1.472543  3.447962  0.277871 -0.587421   \n",
       "\n",
       "       RMSP       YBL      CONP  ...  HFL_6.0  HFL_7.0  HFL_8.0  HFL_9.0  \\\n",
       "0 -1.975683 -0.673670 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "1 -1.089291 -0.072487 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "2  0.240298  2.031653 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "3 -0.202898 -0.072487 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "4  0.240298  0.228104 -0.213087  ...      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   RNTM_0  RNTM_1.0  RNTM_2.0  WATFP_1.0  WATFP_2.0  WATFP_3.0  \n",
       "0     0.0       1.0       0.0        0.0        1.0        0.0  \n",
       "1     0.0       1.0       0.0        1.0        0.0        0.0  \n",
       "2     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "3     0.0       1.0       0.0        0.0        0.0        1.0  \n",
       "4     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "\n",
       "[5 rows x 434 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the calculate_vif code, but it continues to hang and I can't see why.  Beneath the following cell, I start running vif individually and dropping the max column it returns by hand.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(X, thresh=100):\n",
    "    cols = X.columns\n",
    "    variables = np.arange(X.shape[1])\n",
    "    dropped=True\n",
    "    while dropped:\n",
    "        print('Entered while loop')\n",
    "        dropped=False\n",
    "        c = X[cols[variables]].values\n",
    "        print('Created c')\n",
    "        vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "        print('Calculated vif')\n",
    "        maxloc = vif.index(max(vif))\n",
    "        print(f'Max = {maxloc} at {max(vif)}')\n",
    "        if max(vif) > thresh:\n",
    "            print('dropping \\'' + X[cols[variables]].columns[maxloc] + '\\' at index: ' + str(maxloc))\n",
    "            variables = np.delete(variables, maxloc)\n",
    "            dropped=True\n",
    "    print('Remaining variables:')\n",
    "    print(X.columns[variables])\n",
    "    return X[cols[variables]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual vif and column dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 17 at 183.38035314331293\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.275597\n",
       "1         0.318254\n",
       "2        -0.662864\n",
       "3         0.867467\n",
       "4        -0.662864\n",
       "            ...   \n",
       "107418    1.664092\n",
       "107419   -0.662864\n",
       "107420   -0.023004\n",
       "107421   -0.662864\n",
       "107422   -0.662864\n",
       "Name: GRNTP, Length: 107423, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['GRNTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 2 at 7.861800044133322\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        -1.071423\n",
       "1         0.824870\n",
       "2         0.824870\n",
       "3        -1.071423\n",
       "4         1.456967\n",
       "            ...   \n",
       "107418   -1.071423\n",
       "107419   -0.439326\n",
       "107420    2.089065\n",
       "107421    0.824870\n",
       "107422    0.824870\n",
       "Name: NP, Length: 107423, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns = ['NP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created c\n",
      "Calculated vif\n",
      "Max = 4 at 3.5718768321254966\n"
     ]
    }
   ],
   "source": [
    "c = X_train[X_train.columns[np.arange(X.shape[1])]].values\n",
    "print('Created c')\n",
    "vif = [variance_inflation_factor(c, ix) for ix in np.arange(c.shape[1])]\n",
    "print('Calculated vif')\n",
    "maxloc = vif.index(max(vif))\n",
    "print(f'Max = {maxloc} at {max(vif)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After identifying the columns with collinearity issues, I drop them from X_test as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.drop(columns = ['NP', 'GRNTP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HINCP</th>\n",
       "      <th>VEH</th>\n",
       "      <th>NPF</th>\n",
       "      <th>NRC</th>\n",
       "      <th>BDSP</th>\n",
       "      <th>BLD</th>\n",
       "      <th>RMSP</th>\n",
       "      <th>YBL</th>\n",
       "      <th>CONP</th>\n",
       "      <th>ELEP</th>\n",
       "      <th>...</th>\n",
       "      <th>HFL_6.0</th>\n",
       "      <th>HFL_7.0</th>\n",
       "      <th>HFL_8.0</th>\n",
       "      <th>HFL_9.0</th>\n",
       "      <th>RNTM_0</th>\n",
       "      <th>RNTM_1.0</th>\n",
       "      <th>RNTM_2.0</th>\n",
       "      <th>WATFP_1.0</th>\n",
       "      <th>WATFP_2.0</th>\n",
       "      <th>WATFP_3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.121290</td>\n",
       "      <td>-1.709006</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-2.290219</td>\n",
       "      <td>1.549852</td>\n",
       "      <td>-1.975683</td>\n",
       "      <td>-0.673670</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-1.148211</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.620209</td>\n",
       "      <td>0.006660</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>0.440513</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>0.267488</td>\n",
       "      <td>-1.089291</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.170364</td>\n",
       "      <td>0.864493</td>\n",
       "      <td>0.944122</td>\n",
       "      <td>1.442996</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>2.031653</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.687579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001396</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>-1.169563</td>\n",
       "      <td>-0.561970</td>\n",
       "      <td>-1.434189</td>\n",
       "      <td>2.404761</td>\n",
       "      <td>-0.202898</td>\n",
       "      <td>-0.072487</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.503327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.252154</td>\n",
       "      <td>-0.851173</td>\n",
       "      <td>1.472543</td>\n",
       "      <td>3.447962</td>\n",
       "      <td>0.277871</td>\n",
       "      <td>-0.587421</td>\n",
       "      <td>0.240298</td>\n",
       "      <td>0.228104</td>\n",
       "      <td>-0.213087</td>\n",
       "      <td>-0.226948</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 432 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      HINCP       VEH       NPF       NRC      BDSP       BLD      RMSP  \\\n",
       "0 -0.121290 -1.709006 -1.169563 -0.561970 -2.290219  1.549852 -1.975683   \n",
       "1 -0.620209  0.006660  0.944122  0.440513 -1.434189  0.267488 -1.089291   \n",
       "2 -0.170364  0.864493  0.944122  1.442996  0.277871 -0.587421  0.240298   \n",
       "3  0.001396 -0.851173 -1.169563 -0.561970 -1.434189  2.404761 -0.202898   \n",
       "4 -0.252154 -0.851173  1.472543  3.447962  0.277871 -0.587421  0.240298   \n",
       "\n",
       "        YBL      CONP      ELEP  ...  HFL_6.0  HFL_7.0  HFL_8.0  HFL_9.0  \\\n",
       "0 -0.673670 -0.213087 -1.148211  ...      0.0      0.0      0.0      0.0   \n",
       "1 -0.072487 -0.213087 -0.687579  ...      0.0      0.0      0.0      0.0   \n",
       "2  2.031653 -0.213087 -0.687579  ...      0.0      0.0      0.0      0.0   \n",
       "3 -0.072487 -0.213087 -0.503327  ...      0.0      0.0      0.0      0.0   \n",
       "4  0.228104 -0.213087 -0.226948  ...      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   RNTM_0  RNTM_1.0  RNTM_2.0  WATFP_1.0  WATFP_2.0  WATFP_3.0  \n",
       "0     0.0       1.0       0.0        0.0        1.0        0.0  \n",
       "1     0.0       1.0       0.0        1.0        0.0        0.0  \n",
       "2     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "3     0.0       1.0       0.0        0.0        0.0        1.0  \n",
       "4     1.0       0.0       0.0        0.0        0.0        1.0  \n",
       "\n",
       "[5 rows x 432 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running vanilla LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschol/anaconda3/lib/python3.7/site-packages/numpy/lib/arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "def encoder_transform(encoder, X):\n",
    "    X_encoded = encoder.transform(X).toarray()\n",
    "    encoded_feats = list(encoder.get_feature_names())\n",
    "    feats = X.columns\n",
    "    encoded_feats_updated = []\n",
    "    for feat in encoded_feats:\n",
    "        feat_split = feat.split('_')\n",
    "        i = int(feat_split[0][1:])\n",
    "        dummies = feat_split[1]\n",
    "        feat_updated = f'{feats[i]}_{dummies}'\n",
    "        encoded_feats_updated.append(feat_updated)\n",
    "    return pd.DataFrame(X_encoded, columns = encoded_feats_updated)\n",
    "\n",
    "file_dir = '/Users/flatironschol/FIS-Projects/Module5/data/'\n",
    "df = pd.read_csv(f'{file_dir}df.csv', index_col = 0)\n",
    "df_ = df.groupby('FS').apply(lambda x: x.sample(frac = 0.5))\n",
    "df_.index = df_.index.droplevel(0)        \n",
    "# df_ca = df.loc[df.ST == 6]\n",
    "y = df_.FS\n",
    "# X = df_.drop(['FS', 'SERIALNO', 'REGION', 'DIVISION', 'ST', \\\n",
    "#               'HOTWAT', 'RWATPR', 'PLMPRP'], axis = 1)\n",
    "X = df_.drop(['FS', 'SERIALNO', 'REGION', 'DIVISION', 'ST'], axis = 1)\n",
    "# Split the data into test and training samples--stratify by SNAP recipiency\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \\\n",
    "                                                    stratify = y, \\\n",
    "                                                    test_size = 0.2, \\\n",
    "                                                    random_state = 1007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scaling and one-hot-encoding of the training set\n",
    "cont_feats = ['HINCP', 'VEH', 'NP', 'NPF', 'NRC', 'BDSP', 'BLD', 'RMSP', \\\n",
    "              'YBL', 'CONP', 'ELEP', 'GASP', 'FULP', 'INSP', 'MHP', \\\n",
    "              'RNTP', 'WATP', 'GRNTP', 'SMOCP']\n",
    "sclr = StandardScaler()\n",
    "X_train_cont = pd.DataFrame(sclr.fit_transform(X_train[cont_feats]), \\\n",
    "                            columns = cont_feats)\n",
    "cat_feats = X_train.drop(cont_feats, axis = 1).columns\n",
    "encdr = OneHotEncoder(handle_unknown = 'ignore')\n",
    "encdr.fit(X_train[cat_feats])\n",
    "X_train_cat = encoder_transform(encdr, X_train[cat_feats])\n",
    "X_train = pd.concat((X_train_cont, X_train_cat), axis = 1)\n",
    "y_train = y_train.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train.to_csv(f'{file_dir}X_train.csv')\n",
    "# y_train.to_csv(f'{file_dir}y_train.csv')\n",
    "\n",
    "# Scaling and one-hot-encoding of the test set\n",
    "X_test_cont = pd.DataFrame(sclr.transform(X_test[cont_feats]), \\\n",
    "                           columns = cont_feats)\n",
    "X_test_cat = encoder_transform(encdr, X_test[cat_feats])\n",
    "X_test = pd.concat((X_test_cont, X_test_cat), axis = 1)\n",
    "y_test = y_test.astype('int')\n",
    "# X_test.to_csv(f'{file_dir}X_test.csv')\n",
    "# y_test.to_csv(f'{file_dir}y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "y_train = y_train.astype('int')\n",
    "clf = LogisticRegression(random_state=1007, solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.99      0.96     24819\n",
      "           2       0.60      0.19      0.29      2037\n",
      "\n",
      "    accuracy                           0.93     26856\n",
      "   macro avg       0.77      0.59      0.62     26856\n",
      "weighted avg       0.91      0.93      0.91     26856\n",
      "\n",
      "[[24564   255]\n",
      " [ 1655   382]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "y_test = y_test.astype('int')\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not terrific.  Let's try a deep GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'penalty':('l1', 'l2', 'none'), 'C':[0.001, 0.01, 0.1, 1, 10, 100, 1e9]}\n",
    "clfgs = GridSearchCV(clf, parameters, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:1510: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  \"Setting penalty='none' will ignore the C and l1_ratio \"\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                          fit_intercept=True,\n",
       "                                          intercept_scaling=1, l1_ratio=None,\n",
       "                                          max_iter=100, multi_class='warn',\n",
       "                                          n_jobs=None, penalty='l2',\n",
       "                                          random_state=1007, solver='saga',\n",
       "                                          tol=0.0001, verbose=0,\n",
       "                                          warm_start=False),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000000000.0],\n",
       "                         'penalty': ('l1', 'l2', 'none')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.99      0.96     24819\n",
      "           2       0.60      0.19      0.29      2037\n",
      "\n",
      "    accuracy                           0.93     26856\n",
      "   macro avg       0.77      0.59      0.62     26856\n",
      "weighted avg       0.91      0.93      0.91     26856\n",
      "\n",
      "[[24564   255]\n",
      " [ 1655   382]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXACTLY the same.  Not great.  The classes are pretty imbalanced though (around 12:1), so SMOTE may be a good option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE()\n",
    "X_train, y_train = smt.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train == 1)/len(y_train) # shows classes are evenly distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try another vanilla Logreg - the same parameters (C = 1, penalty = l2) came out in GridSearchCV so we'll stick with those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/linear_model/sag.py:337: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=1007, solver='saga').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.79      0.87     24819\n",
      "           2       0.23      0.77      0.36      2037\n",
      "\n",
      "    accuracy                           0.79     26856\n",
      "   macro avg       0.60      0.78      0.61     26856\n",
      "weighted avg       0.92      0.79      0.83     26856\n",
      "\n",
      "[[19573  5246]\n",
      " [  463  1574]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "print(confusion_matrix(y_test, clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now precision is low but recall is much better.  Let's try to visualize the precision-recall curve here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic: f1=0.873 auc=0.387\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict probabilities\n",
    "lr_probs = clf.predict_proba(X_test)\n",
    "# keep probabilities for the positive outcome only\n",
    "lr_probs = lr_probs[:, 1]\n",
    "# predict class values\n",
    "yhat = clf.predict(X_test)\n",
    "y_test = np.array(y_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs, pos_label =2)\n",
    "lr_f1, lr_auc = f1_score(y_test, yhat), auc(lr_recall, lr_precision)\n",
    "# summarize scores\n",
    "print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
